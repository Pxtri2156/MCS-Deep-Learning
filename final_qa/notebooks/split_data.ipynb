{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_sq = '/workspace/tripx/MCS/deep_learning/squad_data_split/train-v2.0.json'\n",
    "dev_path_sq = '/workspace/tripx/MCS/deep_learning/squad_data_split/dev-v2.0.json'\n",
    "\n",
    "\n",
    "def read_data(path):  \n",
    "  # load the json file\n",
    "  with open(path, 'rb') as f:\n",
    "    squad = json.load(f)\n",
    "\n",
    "  contexts = []\n",
    "  questions = []\n",
    "  answers = []\n",
    "\n",
    "  for group in squad['data']:\n",
    "    for passage in group['paragraphs']:\n",
    "      context = passage['context']\n",
    "      for qa in passage['qas']:\n",
    "        question = qa['question']\n",
    "        for answer in qa['answers']:\n",
    "          contexts.append(context)\n",
    "          questions.append(question)\n",
    "          answers.append(answer)\n",
    "\n",
    "  return contexts[:50], questions[:50], answers[:50]\n",
    "\n",
    "train_contexts_sq, train_questions_sq, train_answers_sq = read_data(train_path_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_contexts_sq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When did Beyonce start becoming popular?'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions_sq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'in the late 1990s', 'answer_start': 269}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_answers_sq[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow2.0 Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "path='/workspace/tripx/MCS/deep_learning/final_qa/data/nq-sub-train-v1.0.1.json'\n",
    "\n",
    "f = open(path)\n",
    "data_test = json.load(f)\n",
    "print(type(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_qa = data_test['data'][1]['paragraphs'][0]['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human fertilization - wikipedia <H1> Human fertilization </H1> Jump to : navigation , search <Table> <Tr> <Td> Part of a series on </Td> </Tr> <Tr> <Th> Human growth and development </Th> </Tr> <Tr> <Td> </Td> </Tr> <Tr> <Th> Stages </Th> </Tr> <Tr> <Td> <Ul> <Li> Human embryogenesis </Li> <Li> Fetus </Li> <Li> Infant </Li> <Li> Toddler </Li> <Li> Early childhood </Li> <Li> Child </Li> <Li> Preadolescence </Li> <Li> Adolescence </Li> <Li> Adult </Li> <Li> Middle age </Li> <Li> Old age </Li> </Ul> </Td> </Tr> <Tr> <Th> Biological milestones </Th> </Tr> <Tr> <Td> <Ul> <Li> Fertilization </Li> <Li> Childbirth </Li> <Li> Walking </Li> <Li> Language acquisition </Li> <Li> Puberty </Li> <Li> Menopause </Li> <Li> Ageing </Li> <Li> Death </Li> </Ul> </Td> </Tr> <Tr> <Th> Development and psychology </Th> </Tr> <Tr> <Td> <Ul> <Li> Pre - and perinatal </Li> <Li> Infant and child </Li> <Li> Adolescent </Li> <Li> Youth </Li> <Li> Young adult </Li> <Li> Adult </Li> <Li> Maturity </Li> </Ul> </Td> </Tr> <Tr> <Th> Developmental stage theories </Th> </Tr> <Tr> <Td> <Ul> <Li> Attachment </Li> <Li> Ecological </Li> <Li> Psychosocial </Li> <Li> Psychosexual development </Li> <Li> Moral </Li> <Li> Cognitive </Li> <Li> Cultural - historical </Li> <Li> Evolutionary </Li> </Ul> </Td> </Tr> <Tr> <Td> <Ul> <Li> Human body portal </Li> </Ul> </Td> </Tr> <Tr> <Td> <Ul> <Li> </Li> <Li> </Li> <Li> </Li> </Ul> </Td> </Tr> </Table> The acrosome reaction for a sea urchin , a similar process . Note that the picture shows several stages of one and the same spermatozoon - only one penetrates the ovum Illustration depicting ovulation and fertilization . <P> Human fertilization is the union of a human egg and sperm , usually occurring in the ampulla of the fallopian tube . The result of this union is the production of a zygote cell , or fertilized egg , initiating prenatal development . Scientists discovered the dynamics of human fertilization in the nineteenth century . </P> <P> The process of fertilization involves a sperm fusing with an ovum . The most common sequence begins with ejaculation during copulation , follows with ovulation , and finishes with fertilization . Various exceptions to this sequence are possible , including artificial insemination , in vitro fertilization , external ejaculation without copulation , or copulation shortly after ovulation . Upon encountering the secondary oocyte , the acrosome of the sperm produces enzymes which allow it to burrow through the outer jelly coat of the egg . The sperm plasma then fuses with the egg 's plasma membrane , the sperm head disconnects from its flagellum and the egg travels down the Fallopian tube to reach the uterus . </P> <P> In vitro fertilization ( IVF ) is a process by which egg cells are fertilized by sperm outside the womb , in vitro . </P> <P> </P> <H2> Contents </H2> ( hide ) <Ul> <Li> 1 Anatomy <Ul> <Li> 1.1 Corona radiata </Li> <Li> 1.2 Cone of attraction and perivitelline membrane </Li> <Li> 1.3 Sperm preparation </Li> <Li> 1.4 Zona pellucida <Ul> <Li> 1.4. 1 Cortical reaction </Li> </Ul> </Li> </Ul> </Li> <Li> 2 Fusion <Ul> <Li> 2.1 Cell membranes </Li> <Li> 2.2 Transformations </Li> <Li> 2.3 Replication </Li> <Li> 2.4 Mitosis </Li> </Ul> </Li> <Li> 3 Fertilization age </Li> <Li> 4 Diseases </Li> <Li> 5 See also </Li> <Li> 6 References </Li> <Li> 7 External links </Li> </Ul> <P> </P> <H2> Anatomy ( edit ) </H2> <H3> Corona radiata ( edit ) </H3> <P> The sperm binds through the corona radiata , a layer of follicle cells on the outside of the secondary oocyte . Fertilization occurs when the nucleus of both a sperm and an egg fuse to form a diploid cell , known as zygote . The successful fusion of gametes forms a new organism . </P> <H3> Cone of attraction and perivitelline membrane ( edit ) </H3> <P> Where the spermatozoon is about to pierce , the yolk ( ooplasm ) is drawn out into a conical elevation , termed the cone of attraction or reception cone . Once the spermatozoon has entered , the peripheral portion of the yolk changes into a membrane , the perivitelline membrane , which prevents the passage of additional spermatozoa . </P> <H3> Sperm preparation ( edit ) </H3> Further information : Acrosome reaction <P> At the beginning of the process , the sperm undergoes a series of changes , as freshly ejaculated sperm is unable or poorly able to fertilize . The sperm must undergo capacitation in the female 's reproductive tract over several hours , which increases its motility and destabilizes its membrane , preparing it for the acrosome reaction , the enzymatic penetration of the egg 's tough membrane , the zona pellucida , which surrounds the oocyte . </P> <H3> Zona pellucida ( edit ) </H3> <P> After binding to the corona radiata the sperm reaches the zona pellucida , which is an extra-cellular matrix of glycoproteins . A special complementary molecule on the surface of the sperm head binds to a ZP3 glycoprotein in the zona pellucida . This binding triggers the acrosome to burst , releasing enzymes that help the sperm get through the zona pellucida . </P> <P> Some sperm cells consume their acrosome prematurely on the surface of the egg cell , facilitating the penetration by other sperm cells . As a population , sperm cells have on average 50 % genome similarity so the premature acrosomal reactions aid fertilization by a member of the same cohort . It may be regarded as a mechanism of kin selection . </P> <P> Recent studies have shown that the egg is not passive during this process . </P> Cortical reaction ( edit ) <P> Once the sperm cells find their way past the zona pellucida , the cortical reaction occurs . Cortical granules inside the secondary oocyte fuse with the plasma membrane of the cell , causing enzymes inside these granules to be expelled by exocytosis to the zona pellucida . This in turn causes the glyco - proteins in the zona pellucida to cross-link with each other -- i.e. the enzymes cause the ZP2 to hydrolyse into ZP2f -- making the whole matrix hard and impermeable to sperm . This prevents fertilization of an egg by more than one sperm . The cortical reaction and acrosome reaction are both essential to ensure that only one sperm will fertilize an egg . </P> <H2> Fusion ( edit ) </H2> Fertilization and implantation in humans . <P> After the sperm enters the cytoplasm of the oocyte ( also called ovocyte ) , the tail and the outer coating of the sperm disintegrate and the cortical reaction takes place , preventing other sperm from fertilizing the same egg . The oocyte now undergoes its second meiotic division producing the haploid ovum and releasing a polar body . The sperm nucleus then fuses with the ovum , enabling fusion of their genetic material . </P> <H3> Cell membranes ( edit ) </H3> <P> The fusion of cell membranes of the secondary oocyte and sperm takes place . </P> <H3> Transformations ( edit ) </H3> <P> In preparation for the fusion of their genetic material both the oocyte and the sperm undergo transformations as a reaction to the fusion of cell membranes . </P> <P> The oocyte completes its second meiotic division . This results in a mature ovum . The nucleus of the oocyte is called a pronucleus in this process , to distinguish it from the nuclei that are the result of fertilization . </P> <P> The sperm 's tail and mitochondria degenerate with the formation of the male pronucleus . This is why all mitochondria in humans are of maternal origin . Still , a considerable amount of RNA from the sperm is delivered to the resulting embryo and likely influences embryo development and the phenotype of the offspring . </P> <H3> Replication ( edit ) </H3> <P> The pronuclei migrate toward the center of the oocyte , rapidly replicating their DNA as they do so to prepare the zygote for its first mitotic division . </P> <H3> Mitosis ( edit ) </H3> <P> Usually 23 chromosomes from spermatozoon and 23 chromosomes from egg cell fuse ( half of spermatozoons carry X chromosome and the other half Y chromosome ) . Their membranes dissolve , leaving no barriers between the male and female chromosomes . During this dissolution , a mitotic spindle forms between them . The spindle captures the chromosomes before they disperse in the egg cytoplasm . Upon subsequently undergoing mitosis ( which includes pulling of chromatids towards centrioles in anaphase ) the cell gathers genetic material from the male and female together . Thus , the first mitosis of the union of sperm and oocyte is the actual fusion of their chromosomes . </P> <P> Each of the two daughter cells resulting from that mitosis has one replica of each chromatid that was replicated in the previous stage . Thus , they are genetically identical . </P> <H2> Fertilization age ( edit ) </H2> <P> Fertilization is the event most commonly used to mark the zero point in descriptions of prenatal development of the embryo or fetus . The resultant age is known as fertilization age , fertilizational age , embryonic age , fetal age or ( intrauterine ) developmental ( IUD ) age . </P> <P> Gestational age , in contrast , takes the beginning of the last menstrual period ( LMP ) as the zero point . By convention , gestational age is calculated by adding 14 days to fertilization age and vice versa . In fact , however , fertilization usually occurs within a day of ovulation , which , in turn , occurs on average 14.6 days after the beginning of the preceding menstruation ( LMP ) . There is also considerable variability in this interval , with a 95 % prediction interval of the ovulation of 9 to 20 days after menstruation even for an average woman who has a mean LMP - to - ovulation time of 14.6 . In a reference group representing all women , the 95 % prediction interval of the LMP - to - ovulation is 8.2 to 20.5 days . </P> <P> The average time to birth has been estimated to be 268 days ( 38 weeks and two days ) from ovulation , with a standard deviation of 10 days or coefficient of variation of 3.7 % . </P> <P> Fertilization age is sometimes used postnatally ( after birth ) as well to estimate various risk factors . For example , it is a better predictor than postnatal age for risk of intraventricular hemorrhage in premature babies treated with extracorporeal membrane oxygenation . </P> <H2> Diseases ( edit ) </H2> <P> Various disorders can arise from defects in the fertilization process . </P> <Ul> <Li> Polyspermy results from multiple sperm fertilizing an egg . </Li> </Ul> <P> However , some researchers have found that in rare pairs of fraternal twins , their origin might have been from the fertilization of one egg cell from the mother and eight sperm cells from the father . This possibility has been investigated by computer simulations of the fertilization process . </P> <H2> See also ( edit ) </H2> <Ul> <Li> Spontaneous conception , the unassisted conception of a subsequent child after prior use of assisted reproductive technology </Li> </Ul> <H2> References ( edit ) </H2> <Ol> <Li> Jump up ^ Garrison , Fielding . An Introduction to the History of Medicine , pages 566 - 567 ( Saunders 1921 ) . </Li> <Li> Jump up ^ `` Archived copy '' . Archived from the original on 2011 - 12 - 22 . Retrieved 2016 - 01 - 24 . </Li> <Li> Jump up ^ http://www.americanpregnancy.org/preventingpregnancy/pregnancyfaqmyths.html </Li> <Li> Jump up ^ Lawyers Guide to Forensic Medicine SBN 978 - 1 - 85941 - 159 - 9 By Bernard Knight - Page 188 `` Pregnancy is well known to occur from such external ejaculation ... '' </Li> <Li> Jump up ^ `` Fertilization of the Ovum '' . Gray 's Anatomy . Retrieved 2010 - 10 - 16 . </Li> <Li> Jump up ^ `` Fertilization '' . Retrieved 28 July 2010 . </Li> <Li> Jump up ^ Angier , Natalie ( 2007 - 06 - 12 ) . `` Sleek , Fast and Focused : The Cells That Make Dad Dad '' . The New York Times . </Li> <Li> Jump up ^ Suzanne Wymelenberg , Science and Babies , National Academy Press , page 17 </Li> <Li> Jump up ^ Richard E. Jones and Kristin H. Lopez , Human Reproductive Biology , Third Edition , Elsevier , 2006 , page 238 </Li> <Li> Jump up ^ `` Fertilization : The Cortical Reaction '' . Boundless . Boundless . Retrieved 14 March 2013 . </Li> <Li> Jump up ^ Jodar , M. ; Selvaraju , S. ; Sendler , E. ; Diamond , M.P. ; Krawetz , S.A. ; for the Reproductive Medicine Networks ( 2013 ) . `` The presence , role and clinical use of spermatozoal RNAs '' . Human Reproduction Update. 19 ( 6 ) : 604 -- 624 . doi : 10.1093 / humupd / dmt031 . PMC 3796946 . PMID 23856356 . </Li> <Li> ^ Jump up to : Marieb , Elaine M. Human Anatomy and Physiology , 5th ed . pp. 1119 - 1122 ( 2001 ) . ISBN 0 - 8053 - 4989 - 8 </Li> <Li> Jump up ^ https://www.genderselectionauthority.com/blog/five-facts-about-xx-or-xy </Li> <Li> Jump up ^ Wagner F , Erdösová B , Kylarová D ( December 2004 ) . `` Degradation phase of apoptosis during the early stages of human metanephros development '' . Biomed Pap Med Fac Univ Palacky Olomouc Czech Repub. 148 ( 2 ) : 255 -- 6 . doi : 10.5507 / bp. 2004.054 . PMID 15744391 . </Li> <Li> Jump up ^ Robinson , H.P. ; Fleming , J.E.E. ( 1975 ) . `` A Critical Evaluation of Sonar `` crown - Rump Length '' Measurements `` . BJOG : an International Journal of Obstetrics and Gynaecology. 82 ( 9 ) : 702 -- 710 . doi : 10.1111 / j. 1471 - 0528.1975. tb00710. x . </Li> <Li> ^ Jump up to : Geirsson RT ( May 1991 ) . `` Ultrasound instead of last menstrual period as the basis of gestational age assignment '' . Ultrasound Obstet Gynecol. 1 ( 3 ) : 212 -- 9 . doi : 10.1046 / j. 1469 - 0705.1991. 01030212. x . PMID 12797075 . </Li> <Li> Jump up ^ Derived from a standard deviation in this interval of 2.6 , as given in : Fehring RJ , Schneider M , Raviele K ( 2006 ) . `` Variability in the phases of the menstrual cycle '' . J Obstet Gynecol Neonatal Nurs. 35 ( 3 ) : 376 -- 84 . doi : 10.1111\n"
     ]
    }
   ],
   "source": [
    "print(context_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'how i.met your mother who is the mother',\n",
       "  'short_answers': [{'text': 'Tracy McConnell', 'answer_start': 1183}],\n",
       "  'long_answers': [{'text': \"<P> Tracy McConnell , better known as `` The Mother '' , is the title character from the CBS television sitcom How I Met Your Mother . The show , narrated by Future Ted , tells the story of how Ted Mosby met The Mother . Tracy McConnell appears in 8 episodes from `` Lucky Penny '' to `` The Time Travelers '' as an unseen character ; she was first seen fully in `` Something New '' and was promoted to a main character in season 9 . The Mother is played by Cristin Milioti . </P>\",\n",
       "    'answer_start': 1179}],\n",
       "  'id': '5328212470870865242',\n",
       "  'short_is_impossible': False,\n",
       "  'long_is_impossible': False,\n",
       "  'crop_start': 0}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test['data'][1]['paragraphs'][0]['qas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = data_test['data'][2]['paragraphs'][0]['qas'][0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_answers = data_test['data'][2]['paragraphs'][0]['qas'][0]['short_answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how many players on a box lacrosse team'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "train_path = '/workspace/tripx/MCS/deep_learning/natural-question-answering/nq-sub-train-v1.0.1.json'\n",
    "\n",
    "f = open(train_path)\n",
    "data = json.load(f)\n",
    "print(type(data))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path =  '/workspace/tripx/MCS/deep_learning/final_qa/data/nq-val-v1.0.1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "f = open(val_path)\n",
    "val_data = json.load(f)\n",
    "print(type(val_data))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path =  '/workspace/tripx/MCS/deep_learning/final_qa/data/nq-train-v1.0.1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "f = open(train_path)\n",
    "train_data = json.load(f)\n",
    "print(type(val_data))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296993"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train_data = {\"version\": train_data['version'],\n",
    "                'data': train_data['data'][:50000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train_path='/workspace/tripx/MCS/deep_learning/final_qa/data/nq-sub2-train-v1.0.1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sub_train_path, 'w') as f:\n",
    "    json.dump(sub_train_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_val_data = {\"version\": val_data['version'],\n",
    "                'data': val_data['data'][:250]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_val_path = '/workspace/tripx/MCS/deep_learning/natural-question-answering/nq-sub-val-v1.0.1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sub_val_path, 'w') as f:\n",
    "    json.dump(sub_val_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['version', 'data'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_new_data = {\"version\": data['version'],\n",
    "                'data': data['data'][:1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train_path = '/workspace/tripx/MCS/deep_learning/natural-question-answering/nq-sub-train-v1.0.1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sub_train_path, 'w') as f:\n",
    "    json.dump(sub_new_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'https://en.wikipedia.org//w/index.php?title=Email_marketing&amp;oldid=814071202',\n",
       " 'paragraphs': [{'qas': [{'question': 'which is the most common use of opt-in e-mail marketing',\n",
       "     'short_answers': [{'text': \"a newsletter sent to an advertising firm 's customers\",\n",
       "       'answer_start': 9069}],\n",
       "     'long_answers': [{'text': \"<P> A common example of permission marketing is a newsletter sent to an advertising firm 's customers . Such newsletters inform customers of upcoming events or promotions , or new products . In this type of advertising , a company that wants to send a newsletter to their customers may ask them at the point of purchase if they would like to receive the newsletter . </P>\",\n",
       "       'answer_start': 9021}],\n",
       "     'id': '5655493461695504401',\n",
       "     'short_is_impossible': False,\n",
       "     'long_is_impossible': False,\n",
       "     'crop_start': 418}],\n",
       "   'context': \"with the purpose of enhancing a merchant 's relationship with current or previous customers , encouraging customer loyalty and repeat business , acquiring new customers or convincing current customers to purchase something immediately , and sharing third - party ads . </P> <P> </P> <H2> Contents </H2> ( hide ) <Ul> <Li> 1 History </Li> <Li> 2 Types <Ul> <Li> 2.1 Transactional emails </Li> <Li> 2.2 Direct emails <Ul> <Li> 2.2. 1 Mobile email marketing </Li> </Ul> </Li> </Ul> </Li> <Li> 3 Comparison to traditional mail <Ul> <Li> 3.1 Advantages </Li> <Li> 3.2 Disadvantages </Li> </Ul> </Li> <Li> 4 Opt - in email advertising </Li> <Li> 5 Legal requirements <Ul> <Li> 5.1 Australia </Li> <Li> 5.2 Canada </Li> <Li> 5.3 European Union </Li> <Li> 5.4 United States </Li> </Ul> </Li> <Li> 6 See also </Li> <Li> 7 References </Li> </Ul> <P> </P> <H2> History </H2> <P> Email marketing has evolved rapidly alongside the technological growth of the 21st century . Prior to this growth , when emails were novelties to the majority of customers , email marketing was not as effective . In 1978 , Gary Thuerk of Digital Equipment Corporation ( DEC ) sent out the first mass email to approximately 400 potential clients via the Advanced Research Projects Agency Network ( ARPANET ) . This email resulted in $13 million worth of sales in DEC products , and highlighted the potential of marketing through mass emails . However , as email marketing developed as an effective means of direct communication , users began blocking out content from emails with filters and blocking programs . In order to effectively communicate a message through email , marketers had to develop a way of pushing content through to the end user , without being cut out by automatic filters and spam removing software . This resulted in the birth of triggered marketing emails , which are sent to specific users based on their tracked online browsing patterns . </P> <P> Historically , it has been difficult to measure the effectiveness of marketing campaigns because target markets can not be adequately defined . Email marketing carries the benefit of allowing marketers to identify returns on investment and measure and improve efficiency . Email marketing allows marketers to see feedback from users in real time , and to monitor how effective their campaign is in achieving market penetration , revealing a communication channel 's scope . At the same time , however , it also means that the more personal nature of certain advertising methods , such as television advertisements , can not be captured . </P> <H2> Types </H2> <P> Email marketing can be carried out through different types of emails : </P> <H3> Transactional emails </H3> <P> Transactional emails are usually triggered based on a customer 's action with a company . To be qualified as transactional or relationship messages , these communications ' primary purpose must be `` to facilitate , complete , or confirm a commercial transaction that the recipient has previously agreed to enter into with the sender '' along with a few other narrow definitions of transactional messaging . Triggered transactional messages include dropped basket messages , password reset emails , purchase or order confirmation emails , order status emails , reorder emails , and email receipts . </P> <P> The primary purpose of a transactional email is to convey information regarding the action that triggered it . But , due to their high open rates ( 51.3 % compared to 36.6 % for email newsletters ) , transactional emails are an opportunity to introduce or extend the email relationship with customers or subscribers ; to anticipate and answer questions ; or to cross-sell or up - sell products or services . </P> <P> Many email newsletter software vendors offer transactional email support , which gives companies the ability to include promotional messages within the body of transactional emails . There are also software vendors that offer specialized transactional email marketing services , which include providing targeted and personalized transactional email messages and running specific marketing campaigns ( such as customer referral programs ) . </P> <H3> Direct emails </H3> <P> Direct email involves sending an email solely to communicate a promotional message ( for example , a special offer or a product catalog ) . Companies usually collect a list of customer or prospect email addresses to send direct promotional messages to , or they rent a list of email addresses from service companies . Safe mail marketing is also used . </P> Mobile email marketing <P> Email marketing develops large amounts of traffic through smartphones and tablets . Marketers are researching ways to advertise to more users and to make them view advertising for longer . However , the rate of delivery is still relatively low due to better filtering - out of advertising and users having multiple email accounts for different purposes . Because emails are generated according to the tracked behavior of consumers , it is possible to send advertising which is based on the recipient 's behavior . Because of this , modern email marketing is perceived more often as a pull strategy rather than a push strategy . </P> <H2> Comparison to traditional mail </H2> <P> There are both advantages and disadvantages to using email marketing in comparison to traditional advertising mail . </P> <H3> Advantages </H3> <P> Email marketing is popular with companies for several reasons : </P> <Ul> <Li> An exact return on investment can be tracked ( `` track to basket '' ) and has proven to be high when done properly . Email marketing is often reported as second only to search marketing as the most effective online marketing tactic . </Li> <Li> Email marketing is significantly cheaper and faster than traditional mail , mainly because of the high cost and time required in a traditional mail campaign for producing the artwork , printing , addressing , and mailing . </Li> <Li> Businesses and organizations who send a high volume of emails can use an ESP ( email service provider ) to gather information about the behavior of the recipients . The insights provided by consumer response to email marketing help businesses and organizations understand and make use of consumer behavior . </Li> <Li> Email provides a cost - effective method to test different marketing content , including visual , creative , marketing copy , and multimedia assets . The data gathered by testing in the email channel can then be used across all channels of marketing campaigns , both print and digital . </Li> <Li> Advertisers can reach substantial numbers of email subscribers who have opted in ( i.e. , consented ) to receive the email . </Li> <Li> Almost half of American Internet users check or send email on a typical day , with emails delivered between 1 am and 5 am local time outperforming those sent at other times in open and click rates . </Li> <Li> Email is popular with digital marketers , rising an estimated 15 % in 2009 to £ 292 million in the UK . </Li> <Li> If compared to standard email , direct email marketing produces higher response rate and higher average order value for e-commerce businesses . </Li> </Ul> <H3> Disadvantages </H3> <P> As of mid-2016 email deliverability is still an issue for legitimate marketers . According to the report , legitimate email servers averaged a delivery rate of 73 % in the U.S. ; six percent were filtered as spam , and 22 % were missing . This lags behind other countries : Australia delivers at 90 % , Canada at 89 % , Britain at 88 % , France at 84 % , Germany at 80 % and Brazil at 79 % . </P> <P> Additionally , consumers receive on average circa 90 emails per day . </P> <P> Companies considering the use of an email marketing program must make sure that their program does not violate spam laws such as the United States ' Controlling the Assault of Non-Solicited Pornography and Marketing Act ( CAN - SPAM ) , the European Privacy and Electronic Communications Regulations 2003 , or their Internet service provider 's acceptable use policy . </P> <H2> Opt - in email advertising </H2> <P> Opt - in email advertising , or permission marketing , is a method of advertising via email whereby the recipient of the advertisement has consented to receive it . This method is one of several developed by marketers to eliminate the disadvantages of email marketing . </P> <P> Opt - in email marketing may evolve into a technology that uses a handshake protocol between the sender and receiver . This system is intended to eventually result in a high degree of satisfaction between consumers and marketers . If opt - in email advertising is used , the material that is emailed to consumers will be `` anticipated '' . It is assumed that the recipient wants to receive it , which makes it unlike unsolicited advertisements sent to the consumer . Ideally , opt - in email advertisements will be more personal and relevant to the consumer than untargeted advertisements . </P> <P> A common example of permission marketing is a newsletter sent to an advertising firm 's customers . Such newsletters inform customers of upcoming events or promotions , or new products . In this type of advertising , a company that wants to send a newsletter to their customers may ask them at the point of purchase if they would like to receive the newsletter . </P> <P> With a foundation of opted - in contact information stored in their database , marketers can send out promotional materials automatically using autoresponders -- known as drip marketing . They can also segment their promotions to specific market segments . </P> <H2> Legal requirements </H2> <H3> Australia </H3> <P> The Australian Spam Act 2003 is enforced by the Australian Communications and Media Authority , widely known as `` ACMA '' . The act defines the term unsolicited electronic messages , states how unsubscribe functions must work for commercial messages , and gives other key information . Fines range with 3 fines of AU $110,000 being issued to Virgin Blue Airlines ( 2011 ) , Tiger Airways Holdings Limited ( 2012 ) and Cellar master Wines Pty Limited ( 2013 ) . </P> <H3> Canada </H3> <P> The `` Canada Anti-Spam Law '' ( CASL ) went into effect on July 1 , 2014 . CASL requires an explicit or implicit opt - in from users , and the maximum fines for noncompliance are CA $ 1 million for individuals and $10 million for businesses . </P> <H3> European Union </H3> <P> In 2002 the European Union ( EU ) introduced the Directive on Privacy and Electronic Communications . Article 13 of the Directive prohibits the use of personal email addresses for marketing purposes . The Directive establishes the opt - in regime , where unsolicited emails may be sent only with prior agreement of the recipient ; this does not apply to business email addresses . </P> <P> The directive has since been incorporated into the laws of member states . In the UK it is covered under the Privacy and Electronic Communications ( EC Directive ) Regulations 2003 and applies to all organizations that send out marketing by some form of electronic communication . </P> <H3> United states </H3> <P> The CAN - SPAM Act of 2003 was passed by Congress as a direct response of the growing number of complaints over spam e-mails . Congress determined that the US government was showing an increased interest in the regulation of commercial electronic mail nationally , that those who send commercial e-mails should not mislead recipients over the source or content of them , and that all recipients of such emails have a right to decline them . The act authorizes a US $16,000 penalty per violation for spamming each individual recipient . However , it does not ban spam emailing outright , but imposes laws on using deceptive marketing methods through headings which are `` materially false or misleading '' . In addition there are conditions which email marketers must meet in terms of their format , their content and labeling . As a result , many commercial email marketers within the United States utilize a service or special software to ensure compliance with the act . A variety of older systems exist that do not ensure compliance with the act . To comply with the act 's regulation of commercial email , services also typically require users to authenticate their return address and include a valid physical address , provide a one - click unsubscribe feature , and prohibit importing lists of purchased addresses that may not have given valid permission . </P> <P> In addition to satisfying legal requirements , email service providers ( ESPs ) began to help customers establish and manage their own email marketing campaigns . The service providers supply email templates and general best practices , as well as methods for handling subscriptions and cancellations automatically . Some ESPs will provide insight and assistance with deliverability issues for major email providers . They also provide statistics pertaining to the number of messages received and opened , and whether the recipients clicked on any links within the messages . </P> <P> The CAN - SPAM Act was updated with some new regulations including a no - fee provision for opting out , further definition of `` sender '' , post office or private mail boxes count as a `` valid physical postal address '' and definition of `` person '' . These new provisions went into effect on July 7 , 2008 . </P> <H2> See also </H2> <Ul> <Li> CAUCE -- Coalition Against Unsolicited Commercial Email </Li> <Li> Customer engagement </Li> <Li> Suppression list </Li> <Li> Email spam - Unsolicited email marketing </Li> </Ul> <H2> References </H2> <Ol> <Li> Jump up ^ `` spam unsolicited e-mail '' . Retrieved September 19 , 2016 . </Li> <Li> Jump up ^ `` PUBLIC LAW 108 -- 187 -- DEC . 16 , 2003 117 STAT. 2699 '' ( PDF ) . U.S Government GPO . </Li> <Li> Jump up ^ ADIKESAVAN , T. MANAGEMENT INFORMATION SYSTEMS BEST PRACTICES AND APPLICATIONS IN BUSINESS . ISBN 8120348966 . Retrieved July 10 , 2015 . </Li> <Li> Jump up ^ MECLABS , content : MarketingSherpa , design : Scott McDaniel , code : Steve Beger , ( January 21 , 2009 ) . `` New Survey Data : Email 's ROI Makes Tactic Key for Marketers in 2009 '' . MarketingSherpa.com . Retrieved August 12 , 2017 . </Li> <Li> Jump up ^ Pew Internet & American Life Project , `` Tracking surveys '' , March 2000 -- March 2009 </Li> <Li> Jump up ^ How Scheduling Affects Rates . Mailermailer.com ( July 2012 ) .\"}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'paragraphs'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['data'][0]['paragraphs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_qa = data['data'][0]['paragraphs'][0]['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokens = []\n",
    "char_to_word_offset = []\n",
    "prev_is_whitespace = True\n",
    "for c in context_qa: # get characters\n",
    "    if is_whitespace(c):\n",
    "        prev_is_whitespace = True\n",
    "    else:\n",
    "        if prev_is_whitespace:\n",
    "            doc_tokens.append(c)\n",
    "        else:\n",
    "            doc_tokens[-1] += c\n",
    "        prev_is_whitespace = False\n",
    "    char_to_word_offset.append(len(doc_tokens) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_word_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process qa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = data['data'][0]['paragraphs'][0]['qas'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_position = None\n",
    "end_position = None\n",
    "long_position = None\n",
    "orig_answer_text = None\n",
    "short_is_impossible = False\n",
    "long_is_impossible = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig_answer_text:  <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "short_is_impossible = qa[\"short_is_impossible\"]\n",
    "short_answers = qa[\"short_answers\"]\n",
    "if len(short_answers) >= 2:\n",
    "    # print(f\"Choosing leftmost of \"\n",
    "    #     f\"{len(short_answers)} short answer\")\n",
    "    short_answers = sorted(short_answers, key=lambda sa: sa[\"answer_start\"])\n",
    "    short_answers = short_answers[0: 1]\n",
    "\n",
    "if not short_is_impossible:\n",
    "    answer = short_answers[0]\n",
    "    orig_answer_text = answer[\"text\"]\n",
    "    answer_offset = answer[\"answer_start\"]\n",
    "    answer_length = len(orig_answer_text)\n",
    "    start_position = char_to_word_offset[answer_offset]\n",
    "    end_position = char_to_word_offset[\n",
    "        answer_offset + answer_length - 1]\n",
    "    # Only add answers where the text can be exactly\n",
    "    # recovered from the document. If this CAN'T\n",
    "    # happen it's likely due to weird Unicode stuff\n",
    "    # so we will just skip the example.\n",
    "    #\n",
    "    # Note that this means for training mode, every\n",
    "    # example is NOT guaranteed to be preserved.\n",
    "    actual_text = \" \".join(doc_tokens[start_position:\n",
    "        end_position + 1])\n",
    "    print('orig_answer_text: ', type(orig_answer_text))\n",
    "\n",
    "else:\n",
    "    start_position = -1\n",
    "    end_position = -1\n",
    "    orig_answer_text = \"\"\n",
    "\n",
    "long_is_impossible = qa[\"long_is_impossible\"]\n",
    "long_answers = qa[\"long_answers\"]\n",
    "if (len(long_answers) != 1) and not long_is_impossible:\n",
    "    raise ValueError(f\"For training, each question\"\n",
    "        f\" should have exactly 1 long answer.\")\n",
    "\n",
    "if not long_is_impossible:\n",
    "    long_answer = long_answers[0]\n",
    "    long_answer_offset = long_answer[\"answer_start\"]\n",
    "    long_position = char_to_word_offset[long_answer_offset]\n",
    "else:\n",
    "    long_position = -1\n",
    "\n",
    "# print(f'Q:{question_text}')\n",
    "# print(f'A:{start_position}, {end_position},\n",
    "# {orig_answer_text}')\n",
    "# print(f'R:{doc_tokens[start_position: end_position]}')\n",
    "\n",
    "if not short_is_impossible and not long_is_impossible:\n",
    "    assert long_position <= start_position # nằm sau nên phải lớn hơn\n",
    "\n",
    "if not short_is_impossible and long_is_impossible:\n",
    "    assert False, f'Invalid pair short, long pair'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a newsletter sent to an advertising firm 's customers\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(doc_tokens[start_position:\n",
    "        end_position + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NQExample = collections.namedtuple(\"NQExample\", [\n",
    "    \"qas_id\", \"question_text\", \"doc_tokens\", \"orig_answer_text\",\n",
    "    \"start_position\", \"end_position\", \"long_position\",\n",
    "    \"short_is_impossible\", \"long_is_impossible\", \"crop_start\"])\n",
    "\n",
    "Crop = collections.namedtuple(\"Crop\", [\"unique_id\", \"example_index\", \"doc_span_index\",\n",
    "    \"tokens\", \"token_to_orig_map\", \"token_is_max_context\",\n",
    "    \"input_ids\", \"attention_mask\", \"token_type_ids\",\n",
    "    # \"p_mask\",\n",
    "    \"paragraph_len\", \"start_position\", \"end_position\", \"long_position\",\n",
    "    \"short_is_impossible\", \"long_is_impossible\"])\n",
    "\n",
    "LongAnswerCandidate = collections.namedtuple('LongAnswerCandidate', [\n",
    "    'start_token', 'end_token', 'top_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = NQExample(\n",
    "    qas_id=qa[\"id\"],\n",
    "    question_text=qa[\"question\"],\n",
    "    doc_tokens=doc_tokens,\n",
    "    orig_answer_text=orig_answer_text,\n",
    "    start_position=start_position,\n",
    "    end_position=end_position,\n",
    "    long_position=long_position,\n",
    "    short_is_impossible=short_is_impossible,\n",
    "    long_is_impossible=long_is_impossible,\n",
    "    crop_start=qa[\"crop_start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/tripx/miniconda3/envs/big_data_v2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length =512\n",
    "doc_stride=256\n",
    "max_query_length=64\n",
    "is_training=True\n",
    "cls_token='[CLS]'\n",
    "sep_token='[SEP]'\n",
    "pad_id=0\n",
    "sequence_a_segment_id=0\n",
    "sequence_b_segment_id=1\n",
    "cls_token_segment_id=0\n",
    "pad_token_segment_id=0\n",
    "mask_padding_with_zero=True\n",
    "p_keep_impossible=0.1\n",
    "sep_token_extra=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert question to query\n",
    "query_tokens = tokenizer.tokenize(example.question_text)\n",
    "if len(query_tokens) > max_query_length:\n",
    "            query_tokens = query_tokens[0:max_query_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id = 1000000000\n",
    "num_short_pos, num_short_neg = 0, 0\n",
    "num_long_pos, num_long_neg = 0, 0\n",
    "sub_token_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops = []\n",
    "example_index=0\n",
    "UNMAPPED = -123\n",
    "CLS_INDEX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_to_orig_index = []\n",
    "orig_to_tok_index = []\n",
    "all_doc_tokens = []\n",
    "\n",
    "for i, token in enumerate(example.doc_tokens):\n",
    "    orig_to_tok_index.append(len(all_doc_tokens))\n",
    "    sub_tokens = sub_token_cache.get(token)\n",
    "    if sub_tokens is None:\n",
    "        sub_tokens = tokenizer.tokenize(token)\n",
    "        sub_token_cache[token] = sub_tokens\n",
    "    tok_to_orig_index.extend([i for _ in range(len(sub_tokens))])\n",
    "    all_doc_tokens.extend(sub_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_to_orig_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_token_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_to_tok_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocSpan = collections.namedtuple(\"DocSpan\", [\"start\", \"length\"])\n",
    "def get_spans(doc_stride, max_tokens_for_doc, max_len):\n",
    "    doc_spans = []\n",
    "    start_offset = 0\n",
    "    while start_offset < max_len:\n",
    "        length = max_len - start_offset\n",
    "        if length > max_tokens_for_doc:\n",
    "            length = max_tokens_for_doc\n",
    "        doc_spans.append(DocSpan(start=start_offset, length=length))\n",
    "        if start_offset + length == max_len:\n",
    "            break\n",
    "        start_offset += min(length, doc_stride)\n",
    "    return doc_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_is_max_context(doc_spans, cur_span_index, position):\n",
    "    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
    "\n",
    "    # Because of the sliding window approach taken to scoring documents, a single\n",
    "    # token can appear in multiple documents. E.g.\n",
    "    #  Doc: the man went to the store and bought a gallon of milk\n",
    "    #  Span A: the man went to the\n",
    "    #  Span B: to the store and bought\n",
    "    #  Span C: and bought a gallon of\n",
    "    #  ...\n",
    "    #\n",
    "    # Now the word 'bought' will have two scores from spans B and C. We only\n",
    "    # want to consider the score with \"maximum context\", which we define as\n",
    "    # the *minimum* of its left and right context (the *sum* of left and\n",
    "    # right context will always be the same, of course).\n",
    "    #\n",
    "    # In the example the maximum context for 'bought' would be span C since\n",
    "    # it has 1 left context and 3 right context, while span B has 4 left context\n",
    "    # and 0 right context.\n",
    "    best_score = None\n",
    "    best_span_index = None\n",
    "    for (span_index, doc_span) in enumerate(doc_spans):\n",
    "        end = doc_span.start + doc_span.length - 1\n",
    "        if position < doc_span.start:\n",
    "            continue\n",
    "        if position > end:\n",
    "            continue\n",
    "        num_left_context = position - doc_span.start\n",
    "        num_right_context = end - position\n",
    "        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
    "        if best_score is None or score > best_score:\n",
    "            best_score = score\n",
    "            best_span_index = span_index\n",
    "\n",
    "    return cur_span_index == best_span_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1963\n",
      "1542\n"
     ]
    }
   ],
   "source": [
    "tok_start_position = None\n",
    "tok_end_position = None\n",
    "if is_training and example.short_is_impossible:\n",
    "    tok_start_position = -1\n",
    "    tok_end_position = -1\n",
    "\n",
    "if is_training and not example.short_is_impossible:\n",
    "    tok_start_position = orig_to_tok_index[example.start_position]\n",
    "    print(tok_start_position)\n",
    "    print(example.start_position)\n",
    "    if example.end_position < len(example.doc_tokens) - 1:\n",
    "        tok_end_position = orig_to_tok_index[\n",
    "            example.end_position + 1] - 1\n",
    "    else:\n",
    "        tok_end_position = len(all_doc_tokens) - 1\n",
    "\n",
    "tok_long_position = None\n",
    "if is_training and example.long_is_impossible:\n",
    "    tok_long_position = -1\n",
    "\n",
    "if is_training and not example.long_is_impossible:\n",
    "    tok_long_position = orig_to_tok_index[example.long_position]\n",
    "\n",
    "# For Bert: [CLS] question [SEP] paragraph [SEP]\n",
    "special_tokens_count = 3\n",
    "if sep_token_extra:\n",
    "    # For Roberta: <s> question </s> </s> paragraph </s>\n",
    "    special_tokens_count += 1\n",
    "max_tokens_for_doc = max_seq_length - len(query_tokens) - special_tokens_count\n",
    "assert max_tokens_for_doc > 0\n",
    "# We can have documents that are longer than the maximum\n",
    "# sequence length. To deal with this we do a sliding window\n",
    "# approach, where we take chunks of the up to our max length\n",
    "# with a stride of `doc_stride`.\n",
    "doc_spans = get_spans(doc_stride, max_tokens_for_doc, len(all_doc_tokens))\n",
    "\n",
    "for doc_span_index, doc_span in enumerate(doc_spans):\n",
    "    # Tokens are constructed as: CLS Query SEP Paragraph SEP\n",
    "    tokens = []\n",
    "    token_to_orig_map = UNMAPPED * np.ones((max_seq_length, ), dtype=np.int32)\n",
    "    token_is_max_context = np.zeros((max_seq_length, ), dtype=np.bool_)\n",
    "    token_type_ids = []\n",
    "\n",
    "    # p_mask: mask with 1 for token than cannot be in the\n",
    "    # answer (0 for token which can be in an answer)\n",
    "    # Original TF implem also keep the classification token\n",
    "    # (set to 0) (not sure why...)\n",
    "    # p_mask = []\n",
    "\n",
    "    short_is_impossible = example.short_is_impossible\n",
    "    start_position = None\n",
    "    end_position = None\n",
    "    special_tokens_offset = special_tokens_count - 1\n",
    "    doc_offset = len(query_tokens) + special_tokens_offset\n",
    "    if is_training and not short_is_impossible:\n",
    "        doc_start = doc_span.start\n",
    "        doc_end = doc_span.start + doc_span.length - 1\n",
    "        if not (tok_start_position >= doc_start and tok_end_position <= doc_end):\n",
    "            start_position = 0\n",
    "            end_position = 0\n",
    "            short_is_impossible = True\n",
    "        else:\n",
    "            start_position = tok_start_position - doc_start + doc_offset\n",
    "            end_position = tok_end_position - doc_start + doc_offset\n",
    "\n",
    "    long_is_impossible = example.long_is_impossible\n",
    "    long_position = None\n",
    "    if is_training and not long_is_impossible:\n",
    "        doc_start = doc_span.start\n",
    "        doc_end = doc_span.start + doc_span.length - 1\n",
    "        # out of span\n",
    "        if not (tok_long_position >= doc_start and tok_long_position <= doc_end):\n",
    "            long_position = 0\n",
    "            long_is_impossible = True\n",
    "        else:\n",
    "            long_position = tok_long_position - doc_start + doc_offset\n",
    "\n",
    "    # drop impossible samples\n",
    "    if long_is_impossible:\n",
    "        if np.random.rand() > p_keep_impossible:\n",
    "            continue\n",
    "\n",
    "    # CLS token at the beginning\n",
    "    tokens.append(cls_token)\n",
    "    token_type_ids.append(cls_token_segment_id)\n",
    "    # p_mask.append(0)  # can be answer\n",
    "\n",
    "    # Query\n",
    "    tokens += query_tokens\n",
    "    token_type_ids += [sequence_a_segment_id] * len(query_tokens)\n",
    "    # p_mask += [1] * len(query_tokens)  # can not be answer\n",
    "\n",
    "    # SEP token\n",
    "    tokens.append(sep_token)\n",
    "    token_type_ids.append(sequence_a_segment_id)\n",
    "    # p_mask.append(1)  # can not be answer\n",
    "    if sep_token_extra:\n",
    "        tokens.append(sep_token)\n",
    "        token_type_ids.append(sequence_a_segment_id)\n",
    "        # p_mask.append(1)\n",
    "\n",
    "    # Paragraph\n",
    "    for i in range(doc_span.length):\n",
    "        split_token_index = doc_span.start + i\n",
    "        # We add `example.crop_start` as the original document\n",
    "        # is already shifted\n",
    "        token_to_orig_map[len(tokens)] = tok_to_orig_index[\n",
    "            split_token_index] + example.crop_start\n",
    "\n",
    "        token_is_max_context[len(tokens)] = check_is_max_context(doc_spans,\n",
    "            doc_span_index, split_token_index)\n",
    "        tokens.append(all_doc_tokens[split_token_index])\n",
    "        token_type_ids.append(sequence_b_segment_id)\n",
    "        # p_mask.append(0)  # can be answer\n",
    "\n",
    "    paragraph_len = doc_span.length\n",
    "\n",
    "    # SEP token\n",
    "    tokens.append(sep_token)\n",
    "    token_type_ids.append(sequence_b_segment_id)\n",
    "    # p_mask.append(1)  # can not be answer\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(pad_id)\n",
    "        attention_mask.append(0 if mask_padding_with_zero else 1)\n",
    "        token_type_ids.append(pad_token_segment_id)\n",
    "        # p_mask.append(1)  # can not be answer\n",
    "\n",
    "    # reduce memory, only input_ids needs more bits\n",
    "    input_ids = np.array(input_ids, dtype=np.int32)\n",
    "    attention_mask = np.array(attention_mask, dtype=np.bool_)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=np.uint8)\n",
    "    # p_mask = np.array(p_mask, dtype=np.bool)\n",
    "\n",
    "    if is_training and short_is_impossible:\n",
    "        start_position = CLS_INDEX\n",
    "        end_position = CLS_INDEX\n",
    "\n",
    "    if is_training and long_is_impossible:\n",
    "        long_position = CLS_INDEX\n",
    "\n",
    "    if example_index in (0, 10):\n",
    "        # too spammy otherwise\n",
    "        if doc_span_index in (0, 5):\n",
    "            print(\"*** Example ***\")\n",
    "            print(\"unique_id: %s\" % (unique_id))\n",
    "            print(\"example_index: %s\" % (example_index))\n",
    "            print(\"doc_span_index: %s\" % (doc_span_index))\n",
    "            print(\"tokens: %s\" % \" \".join(tokens))\n",
    "            # print(\"token_to_orig_map: %s\" % \" \".join([\n",
    "            #     \"%d:%d\" % (x, y) for (x, y) in enumerate(token_to_orig_map)]))\n",
    "            # print(\"token_is_max_context: %s\" % \" \".join([\n",
    "            #     \"%d:%s\" % (x, y) for (x, y) in enumerate(token_is_max_context)\n",
    "            # ]))\n",
    "            print(\"input_ids: %s\" % input_ids)\n",
    "            print(\"attention_mask: %s\" % np.uint8(attention_mask))\n",
    "            print(\"token_type_ids: %s\" % token_type_ids)\n",
    "            if is_training and short_is_impossible:\n",
    "                print(\"short impossible example\")\n",
    "            if is_training and long_is_impossible:\n",
    "                print(\"long impossible example\")\n",
    "            if is_training and not short_is_impossible:\n",
    "                answer_text = \" \".join(tokens[start_position: end_position + 1])\n",
    "                print(\"start_position: %d\" % (start_position))\n",
    "                print(\"end_position: %d\" % (end_position))\n",
    "                print(\"answer: %s\" % (answer_text))\n",
    "\n",
    "    if short_is_impossible:\n",
    "        num_short_neg += 1\n",
    "    else:\n",
    "        num_short_pos += 1\n",
    "\n",
    "    if long_is_impossible:\n",
    "        num_long_neg += 1\n",
    "    else:\n",
    "        num_long_pos += 1\n",
    "\n",
    "    crop = Crop(\n",
    "        unique_id=unique_id,\n",
    "        example_index=example_index,\n",
    "        doc_span_index=doc_span_index,\n",
    "        tokens=tokens,\n",
    "        token_to_orig_map=token_to_orig_map,\n",
    "        token_is_max_context=token_is_max_context,\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids,\n",
    "        # p_mask=p_mask,\n",
    "        paragraph_len=paragraph_len,\n",
    "        start_position=start_position,\n",
    "        end_position=end_position,\n",
    "        long_position=long_position,\n",
    "        short_is_impossible=short_is_impossible,\n",
    "        long_is_impossible=long_is_impossible)\n",
    "    crops.append(crop)\n",
    "    unique_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with',\n",
       " 'the',\n",
       " 'purpose',\n",
       " 'of',\n",
       " 'enhancing',\n",
       " 'a',\n",
       " 'merchant',\n",
       " \"'\",\n",
       " 's',\n",
       " 'relationship',\n",
       " 'with',\n",
       " 'current',\n",
       " 'or',\n",
       " 'previous',\n",
       " 'customers',\n",
       " ',',\n",
       " 'encouraging',\n",
       " 'customer',\n",
       " 'loyalty',\n",
       " 'and',\n",
       " 'repeat',\n",
       " 'business',\n",
       " ',',\n",
       " 'acquiring',\n",
       " 'new',\n",
       " 'customers',\n",
       " 'or',\n",
       " 'convincing',\n",
       " 'current',\n",
       " 'customers',\n",
       " 'to',\n",
       " 'purchase',\n",
       " 'something',\n",
       " 'immediately',\n",
       " ',',\n",
       " 'and',\n",
       " 'sharing',\n",
       " 'third',\n",
       " '-',\n",
       " 'party',\n",
       " 'ads',\n",
       " '.',\n",
       " '<',\n",
       " '/',\n",
       " 'p',\n",
       " '>',\n",
       " '<',\n",
       " 'p',\n",
       " '>',\n",
       " '<',\n",
       " '/',\n",
       " 'p',\n",
       " '>',\n",
       " '<',\n",
       " 'h',\n",
       " '##2',\n",
       " '>',\n",
       " 'contents',\n",
       " '<',\n",
       " '/',\n",
       " 'h',\n",
       " '##2',\n",
       " '>',\n",
       " '(',\n",
       " 'hide',\n",
       " ')',\n",
       " '<',\n",
       " 'ul',\n",
       " '>',\n",
       " '<',\n",
       " 'li',\n",
       " '>',\n",
       " '1',\n",
       " 'history',\n",
       " '<',\n",
       " '/',\n",
       " 'li',\n",
       " '>',\n",
       " '<',\n",
       " 'li',\n",
       " '>',\n",
       " '2',\n",
       " 'types',\n",
       " '<',\n",
       " 'ul',\n",
       " '>',\n",
       " '<',\n",
       " 'li',\n",
       " '>',\n",
       " '2',\n",
       " '.',\n",
       " '1',\n",
       " 'transaction',\n",
       " '##al',\n",
       " 'emails',\n",
       " '<',\n",
       " '/',\n",
       " 'li',\n",
       " '>',\n",
       " '<',\n",
       " 'li',\n",
       " '>',\n",
       " '2',\n",
       " '.',\n",
       " '2',\n",
       " 'direct',\n",
       " 'emails',\n",
       " '<',\n",
       " 'ul',\n",
       " '>',\n",
       " '<',\n",
       " 'li',\n",
       " '>',\n",
       " '2',\n",
       " '.',\n",
       " '2',\n",
       " '.',\n",
       " '1',\n",
       " 'mobile',\n",
       " 'email',\n",
       " 'marketing',\n",
       " '<',\n",
       " '/',\n",
       " 'li',\n",
       " '>',\n",
       " '<',\n",
       " '/',\n",
       " 'ul',\n",
       " '>',\n",
       " '<',\n",
       " '/',\n",
       " 'li',\n",
       " '>',\n",
       " '<',\n",
       " '/',\n",
       " 'ul',\n",
       " '>',\n",
       " '<',\n",
       " '/',\n",
       " 'li',\n",
       " '>',\n",
       " '<',\n",
       " 'li',\n",
       " '>',\n",
       " '3',\n",
       " 'comparison',\n",
       " 'to',\n",
       " 'traditional',\n",
       " 'mail',\n",
       " '<',\n",
       " 'ul',\n",
       " '>',\n",
       " '<',\n",
       " 'li',\n",
       " '>',\n",
       " '3',\n",
       " '.',\n",
       " '1',\n",
       " 'advantages',\n",
       " '<',\n",
       " '/',\n",
       " 'li',\n",
       " '>',\n",
       " '<',\n",
       " 'li',\n",
       " '>',\n",
       " '3',\n",
       " '.',\n",
       " '2',\n",
       " 'disadvantage',\n",
       " '##s',\n",
       " '<',\n",
       " '/',\n",
       " 'li',\n",
       " '>',\n",
       " '<',\n",
       " '/',\n",
       " 'ul',\n",
       " '>',\n",
       " '<',\n",
       " '/',\n",
       " 'li',\n",
       " '>',\n",
       " '<',\n",
       " 'li',\n",
       " '>',\n",
       " '4',\n",
       " 'opt',\n",
       " '-',\n",
       " 'in',\n",
       " 'email',\n",
       " 'advertising',\n",
       " '<',\n",
       " '/',\n",
       " 'li',\n",
       " '>',\n",
       " '<',\n",
       " 'li',\n",
       " '>',\n",
       " '5',\n",
       " 'legal',\n",
       " 'requirements',\n",
       " '<',\n",
       " 'ul',\n",
       " '>',\n",
       " '<',\n",
       " 'li',\n",
       " '>',\n",
       " '5',\n",
       " '.',\n",
       " '1',\n",
       " 'australia',\n",
       " '<',\n",
       " '/',\n",
       " 'li',\n",
       " '>',\n",
       " '<',\n",
       " 'li',\n",
       " '>',\n",
       " '5',\n",
       " '.',\n",
       " '2',\n",
       " 'canada',\n",
       " '<',\n",
       " '/',\n",
       " 'li',\n",
       " '>',\n",
       " '<',\n",
       " 'li',\n",
       " '>',\n",
       " '5',\n",
       " '.',\n",
       " '3',\n",
       " 'european',\n",
       " 'union',\n",
       " '<',\n",
       " '/',\n",
       " 'li',\n",
       " '>',\n",
       " '<',\n",
       " 'li',\n",
       " '>',\n",
       " '5',\n",
       " '.',\n",
       " '4',\n",
       " 'united',\n",
       " 'states',\n",
       " '<',\n",
       " '/',\n",
       " 'li',\n",
       " '>',\n",
       " '<',\n",
       " '/',\n",
       " 'ul',\n",
       " '>',\n",
       " '<',\n",
       " '/',\n",
       " 'li',\n",
       " '>',\n",
       " '<',\n",
       " 'li',\n",
       " '>',\n",
       " '6',\n",
       " 'see',\n",
       " 'also',\n",
       " '<',\n",
       " '/',\n",
       " 'li',\n",
       " '>',\n",
       " '<',\n",
       " 'li',\n",
       " '>',\n",
       " '7',\n",
       " 'references',\n",
       " '<',\n",
       " '/',\n",
       " 'li',\n",
       " '>',\n",
       " '<',\n",
       " '/',\n",
       " 'ul',\n",
       " '>',\n",
       " '<',\n",
       " 'p',\n",
       " '>',\n",
       " '<',\n",
       " '/',\n",
       " 'p',\n",
       " '>',\n",
       " '<',\n",
       " 'h',\n",
       " '##2',\n",
       " '>',\n",
       " 'history',\n",
       " '<',\n",
       " '/',\n",
       " 'h',\n",
       " '##2',\n",
       " '>',\n",
       " '<',\n",
       " 'p',\n",
       " '>',\n",
       " 'email',\n",
       " 'marketing',\n",
       " 'has',\n",
       " 'evolved',\n",
       " 'rapidly',\n",
       " 'alongside',\n",
       " 'the',\n",
       " 'technological',\n",
       " 'growth',\n",
       " 'of',\n",
       " 'the',\n",
       " '21st',\n",
       " 'century',\n",
       " '.',\n",
       " 'prior',\n",
       " 'to',\n",
       " 'this',\n",
       " 'growth',\n",
       " ',',\n",
       " 'when',\n",
       " 'emails',\n",
       " 'were',\n",
       " 'novel',\n",
       " '##ties',\n",
       " 'to',\n",
       " 'the',\n",
       " 'majority',\n",
       " 'of',\n",
       " 'customers',\n",
       " ',',\n",
       " 'email',\n",
       " 'marketing',\n",
       " 'was',\n",
       " 'not',\n",
       " 'as',\n",
       " 'effective',\n",
       " '.',\n",
       " 'in',\n",
       " '1978',\n",
       " ',',\n",
       " 'gary',\n",
       " 'th',\n",
       " '##uer',\n",
       " '##k',\n",
       " 'of',\n",
       " 'digital',\n",
       " 'equipment',\n",
       " 'corporation',\n",
       " '(',\n",
       " 'dec',\n",
       " ')',\n",
       " 'sent',\n",
       " 'out',\n",
       " 'the',\n",
       " 'first',\n",
       " 'mass',\n",
       " 'email',\n",
       " 'to',\n",
       " 'approximately',\n",
       " '400',\n",
       " 'potential',\n",
       " 'clients',\n",
       " 'via',\n",
       " 'the',\n",
       " 'advanced',\n",
       " 'research',\n",
       " 'projects',\n",
       " 'agency',\n",
       " 'network',\n",
       " '(',\n",
       " 'ar',\n",
       " '##pan',\n",
       " '##et',\n",
       " ')',\n",
       " '.',\n",
       " 'this',\n",
       " 'email',\n",
       " 'resulted',\n",
       " 'in',\n",
       " '$',\n",
       " '13',\n",
       " 'million',\n",
       " 'worth',\n",
       " 'of',\n",
       " 'sales',\n",
       " 'in',\n",
       " 'dec',\n",
       " 'products',\n",
       " ',',\n",
       " 'and',\n",
       " 'highlighted',\n",
       " 'the',\n",
       " 'potential',\n",
       " 'of',\n",
       " 'marketing',\n",
       " 'through',\n",
       " 'mass',\n",
       " 'emails',\n",
       " '.',\n",
       " 'however',\n",
       " ',',\n",
       " 'as',\n",
       " 'email',\n",
       " 'marketing',\n",
       " 'developed',\n",
       " 'as',\n",
       " 'an',\n",
       " 'effective',\n",
       " 'means',\n",
       " 'of',\n",
       " 'direct',\n",
       " 'communication',\n",
       " ',',\n",
       " 'users',\n",
       " 'began',\n",
       " 'blocking',\n",
       " 'out',\n",
       " 'content',\n",
       " 'from',\n",
       " 'emails',\n",
       " 'with',\n",
       " 'filters',\n",
       " 'and',\n",
       " 'blocking',\n",
       " 'programs',\n",
       " '.',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'effectively',\n",
       " 'communicate',\n",
       " 'a',\n",
       " 'message',\n",
       " 'through',\n",
       " 'email',\n",
       " ',',\n",
       " 'market',\n",
       " '##ers',\n",
       " 'had',\n",
       " 'to',\n",
       " 'develop',\n",
       " 'a',\n",
       " 'way',\n",
       " 'of',\n",
       " 'pushing',\n",
       " 'content',\n",
       " 'through',\n",
       " 'to',\n",
       " 'the',\n",
       " 'end',\n",
       " 'user',\n",
       " ',',\n",
       " 'without',\n",
       " 'being',\n",
       " 'cut',\n",
       " 'out',\n",
       " 'by',\n",
       " 'automatic',\n",
       " 'filters',\n",
       " 'and',\n",
       " 'spa',\n",
       " '##m',\n",
       " 'removing',\n",
       " 'software',\n",
       " '.',\n",
       " 'this',\n",
       " 'resulted',\n",
       " 'in',\n",
       " 'the',\n",
       " 'birth',\n",
       " 'of',\n",
       " 'triggered',\n",
       " 'marketing',\n",
       " 'emails',\n",
       " ',',\n",
       " 'which',\n",
       " 'are',\n",
       " 'sent',\n",
       " 'to',\n",
       " 'specific',\n",
       " 'users',\n",
       " 'based',\n",
       " 'on',\n",
       " 'their',\n",
       " 'tracked',\n",
       " 'online',\n",
       " 'brows',\n",
       " '##ing',\n",
       " 'patterns',\n",
       " '.',\n",
       " '<',\n",
       " '/',\n",
       " 'p',\n",
       " '>',\n",
       " '<',\n",
       " 'p',\n",
       " '>',\n",
       " 'historically',\n",
       " ',',\n",
       " 'it',\n",
       " 'has',\n",
       " 'been',\n",
       " 'difficult',\n",
       " 'to',\n",
       " 'measure',\n",
       " 'the',\n",
       " 'effectiveness',\n",
       " 'of',\n",
       " 'marketing',\n",
       " 'campaigns',\n",
       " 'because',\n",
       " 'target',\n",
       " 'markets',\n",
       " 'can',\n",
       " 'not',\n",
       " 'be',\n",
       " 'adequately',\n",
       " 'defined',\n",
       " '.',\n",
       " 'email',\n",
       " 'marketing',\n",
       " 'carries',\n",
       " 'the',\n",
       " 'benefit',\n",
       " 'of',\n",
       " 'allowing',\n",
       " 'market',\n",
       " '##ers',\n",
       " 'to',\n",
       " 'identify',\n",
       " 'returns',\n",
       " 'on',\n",
       " 'investment',\n",
       " 'and',\n",
       " 'measure',\n",
       " 'and',\n",
       " 'improve',\n",
       " 'efficiency',\n",
       " '.',\n",
       " 'email',\n",
       " 'marketing',\n",
       " 'allows',\n",
       " 'market',\n",
       " '##ers',\n",
       " 'to',\n",
       " 'see',\n",
       " 'feedback',\n",
       " 'from',\n",
       " 'users',\n",
       " 'in',\n",
       " 'real',\n",
       " 'time',\n",
       " ',',\n",
       " 'and',\n",
       " 'to',\n",
       " 'monitor',\n",
       " 'how',\n",
       " 'effective',\n",
       " 'their',\n",
       " 'campaign',\n",
       " 'is',\n",
       " 'in',\n",
       " 'achieving',\n",
       " 'market',\n",
       " 'penetration',\n",
       " ',',\n",
       " 'revealing',\n",
       " 'a',\n",
       " 'communication',\n",
       " 'channel',\n",
       " \"'\",\n",
       " 's',\n",
       " 'scope',\n",
       " '.',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " ',',\n",
       " 'however',\n",
       " ',',\n",
       " 'it',\n",
       " 'also',\n",
       " 'means',\n",
       " 'that',\n",
       " 'the',\n",
       " 'more',\n",
       " 'personal',\n",
       " 'nature',\n",
       " 'of',\n",
       " 'certain',\n",
       " 'advertising',\n",
       " 'methods',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'television',\n",
       " 'advertisements',\n",
       " ',',\n",
       " 'can',\n",
       " 'not',\n",
       " 'be',\n",
       " 'captured',\n",
       " '.',\n",
       " '<',\n",
       " '/',\n",
       " 'p',\n",
       " '>',\n",
       " '<',\n",
       " 'h',\n",
       " '##2',\n",
       " '>',\n",
       " 'types',\n",
       " '<',\n",
       " '/',\n",
       " 'h',\n",
       " '##2',\n",
       " '>',\n",
       " '<',\n",
       " 'p',\n",
       " '>',\n",
       " 'email',\n",
       " 'marketing',\n",
       " 'can',\n",
       " 'be',\n",
       " 'carried',\n",
       " 'out',\n",
       " 'through',\n",
       " 'different',\n",
       " 'types',\n",
       " 'of',\n",
       " 'emails',\n",
       " ':',\n",
       " '<',\n",
       " '/',\n",
       " 'p',\n",
       " '>',\n",
       " '<',\n",
       " 'h',\n",
       " '##3',\n",
       " '>',\n",
       " 'transaction',\n",
       " '##al',\n",
       " 'emails',\n",
       " '<',\n",
       " '/',\n",
       " 'h',\n",
       " '##3',\n",
       " '>',\n",
       " '<',\n",
       " 'p',\n",
       " '>',\n",
       " 'transaction',\n",
       " '##al',\n",
       " 'emails',\n",
       " 'are',\n",
       " 'usually',\n",
       " 'triggered',\n",
       " 'based',\n",
       " 'on',\n",
       " 'a',\n",
       " 'customer',\n",
       " \"'\",\n",
       " 's',\n",
       " 'action',\n",
       " 'with',\n",
       " 'a',\n",
       " 'company',\n",
       " '.',\n",
       " 'to',\n",
       " 'be',\n",
       " 'qualified',\n",
       " 'as',\n",
       " 'transaction',\n",
       " '##al',\n",
       " 'or',\n",
       " 'relationship',\n",
       " 'messages',\n",
       " ',',\n",
       " 'these',\n",
       " 'communications',\n",
       " \"'\",\n",
       " 'primary',\n",
       " 'purpose',\n",
       " 'must',\n",
       " 'be',\n",
       " '`',\n",
       " '`',\n",
       " 'to',\n",
       " 'facilitate',\n",
       " ',',\n",
       " 'complete',\n",
       " ',',\n",
       " 'or',\n",
       " 'confirm',\n",
       " 'a',\n",
       " 'commercial',\n",
       " 'transaction',\n",
       " 'that',\n",
       " 'the',\n",
       " 'recipient',\n",
       " 'has',\n",
       " 'previously',\n",
       " 'agreed',\n",
       " 'to',\n",
       " 'enter',\n",
       " 'into',\n",
       " 'with',\n",
       " 'the',\n",
       " 'send',\n",
       " '##er',\n",
       " \"'\",\n",
       " \"'\",\n",
       " 'along',\n",
       " 'with',\n",
       " 'a',\n",
       " 'few',\n",
       " 'other',\n",
       " 'narrow',\n",
       " 'definitions',\n",
       " 'of',\n",
       " 'transaction',\n",
       " '##al',\n",
       " 'messaging',\n",
       " '.',\n",
       " 'triggered',\n",
       " 'transaction',\n",
       " '##al',\n",
       " 'messages',\n",
       " 'include',\n",
       " 'dropped',\n",
       " 'basket',\n",
       " 'messages',\n",
       " ',',\n",
       " 'password',\n",
       " 'reset',\n",
       " 'emails',\n",
       " ',',\n",
       " 'purchase',\n",
       " 'or',\n",
       " 'order',\n",
       " 'confirmation',\n",
       " 'emails',\n",
       " ',',\n",
       " 'order',\n",
       " 'status',\n",
       " 'emails',\n",
       " ',',\n",
       " 're',\n",
       " '##ord',\n",
       " '##er',\n",
       " 'emails',\n",
       " ',',\n",
       " 'and',\n",
       " 'email',\n",
       " 'receipts',\n",
       " '.',\n",
       " '<',\n",
       " '/',\n",
       " 'p',\n",
       " '>',\n",
       " '<',\n",
       " 'p',\n",
       " '>',\n",
       " 'the',\n",
       " 'primary',\n",
       " 'purpose',\n",
       " 'of',\n",
       " 'a',\n",
       " 'transaction',\n",
       " '##al',\n",
       " 'email',\n",
       " 'is',\n",
       " 'to',\n",
       " 'convey',\n",
       " 'information',\n",
       " 'regarding',\n",
       " 'the',\n",
       " 'action',\n",
       " 'that',\n",
       " 'triggered',\n",
       " 'it',\n",
       " '.',\n",
       " 'but',\n",
       " ',',\n",
       " 'due',\n",
       " 'to',\n",
       " 'their',\n",
       " 'high',\n",
       " 'open',\n",
       " 'rates',\n",
       " '(',\n",
       " '51',\n",
       " '.',\n",
       " '3',\n",
       " '%',\n",
       " 'compared',\n",
       " 'to',\n",
       " '36',\n",
       " '.',\n",
       " '6',\n",
       " '%',\n",
       " 'for',\n",
       " 'email',\n",
       " 'newsletter',\n",
       " '##s',\n",
       " ')',\n",
       " ',',\n",
       " 'transaction',\n",
       " '##al',\n",
       " 'emails',\n",
       " 'are',\n",
       " 'an',\n",
       " 'opportunity',\n",
       " 'to',\n",
       " 'introduce',\n",
       " 'or',\n",
       " 'extend',\n",
       " 'the',\n",
       " 'email',\n",
       " 'relationship',\n",
       " 'with',\n",
       " 'customers',\n",
       " 'or',\n",
       " 'subscribers',\n",
       " ';',\n",
       " 'to',\n",
       " 'anti',\n",
       " '##ci',\n",
       " '##pate',\n",
       " 'and',\n",
       " 'answer',\n",
       " 'questions',\n",
       " ';',\n",
       " 'or',\n",
       " 'to',\n",
       " 'cross',\n",
       " '-',\n",
       " 'sell',\n",
       " 'or',\n",
       " 'up',\n",
       " '-',\n",
       " 'sell',\n",
       " 'products',\n",
       " 'or',\n",
       " 'services',\n",
       " '.',\n",
       " '<',\n",
       " '/',\n",
       " 'p',\n",
       " '>',\n",
       " '<',\n",
       " 'p',\n",
       " '>',\n",
       " 'many',\n",
       " 'email',\n",
       " 'newsletter',\n",
       " 'software',\n",
       " 'vendors',\n",
       " 'offer',\n",
       " 'transaction',\n",
       " '##al',\n",
       " 'email',\n",
       " 'support',\n",
       " ',',\n",
       " 'which',\n",
       " 'gives',\n",
       " 'companies',\n",
       " 'the',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'include',\n",
       " 'promotional',\n",
       " 'messages',\n",
       " 'within',\n",
       " 'the',\n",
       " 'body',\n",
       " 'of',\n",
       " 'transaction',\n",
       " '##al',\n",
       " 'emails',\n",
       " '.',\n",
       " 'there',\n",
       " 'are',\n",
       " 'also',\n",
       " 'software',\n",
       " 'vendors',\n",
       " 'that',\n",
       " 'offer',\n",
       " 'specialized',\n",
       " 'transaction',\n",
       " '##al',\n",
       " 'email',\n",
       " 'marketing',\n",
       " 'services',\n",
       " ',',\n",
       " 'which',\n",
       " 'include',\n",
       " 'providing',\n",
       " 'targeted',\n",
       " 'and',\n",
       " 'personal',\n",
       " '##ized',\n",
       " 'transaction',\n",
       " '##al',\n",
       " 'email',\n",
       " 'messages',\n",
       " 'and',\n",
       " 'running',\n",
       " 'specific',\n",
       " 'marketing',\n",
       " 'campaigns',\n",
       " '(',\n",
       " 'such',\n",
       " 'as',\n",
       " 'customer',\n",
       " 'refer',\n",
       " '##ral',\n",
       " 'programs',\n",
       " ')',\n",
       " '.',\n",
       " '<',\n",
       " '/',\n",
       " 'p',\n",
       " '>',\n",
       " '<',\n",
       " 'h',\n",
       " '##3',\n",
       " '>',\n",
       " 'direct',\n",
       " 'emails',\n",
       " '<',\n",
       " '/',\n",
       " 'h',\n",
       " '##3',\n",
       " '>',\n",
       " '<',\n",
       " 'p',\n",
       " '>',\n",
       " 'direct',\n",
       " 'email',\n",
       " 'involves',\n",
       " 'sending',\n",
       " 'an',\n",
       " 'email',\n",
       " 'solely',\n",
       " 'to',\n",
       " 'communicate',\n",
       " 'a',\n",
       " 'promotional',\n",
       " 'message',\n",
       " '(',\n",
       " 'for',\n",
       " 'example',\n",
       " ',',\n",
       " 'a',\n",
       " 'special',\n",
       " 'offer',\n",
       " 'or',\n",
       " 'a',\n",
       " 'product',\n",
       " 'catalog',\n",
       " ')',\n",
       " '.',\n",
       " 'companies',\n",
       " 'usually',\n",
       " 'collect',\n",
       " 'a',\n",
       " 'list',\n",
       " 'of',\n",
       " 'customer',\n",
       " 'or',\n",
       " 'prospect',\n",
       " 'email',\n",
       " 'addresses',\n",
       " 'to',\n",
       " 'send',\n",
       " 'direct',\n",
       " 'promotional',\n",
       " 'messages',\n",
       " 'to',\n",
       " ',',\n",
       " 'or',\n",
       " 'they',\n",
       " 'rent',\n",
       " 'a',\n",
       " 'list',\n",
       " 'of',\n",
       " 'email',\n",
       " 'addresses',\n",
       " 'from',\n",
       " 'service',\n",
       " 'companies',\n",
       " '.',\n",
       " 'safe',\n",
       " 'mail',\n",
       " 'marketing',\n",
       " 'is',\n",
       " ...]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocSpan(start=0, length=495),\n",
       " DocSpan(start=256, length=495),\n",
       " DocSpan(start=512, length=495),\n",
       " DocSpan(start=768, length=495),\n",
       " DocSpan(start=1024, length=495),\n",
       " DocSpan(start=1280, length=495),\n",
       " DocSpan(start=1536, length=495),\n",
       " DocSpan(start=1792, length=495),\n",
       " DocSpan(start=2048, length=495),\n",
       " DocSpan(start=2304, length=495),\n",
       " DocSpan(start=2560, length=495),\n",
       " DocSpan(start=2816, length=393)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Crop(unique_id=1000000001, example_index=0, doc_span_index=7, tokens=['[CLS]', 'which', 'is', 'the', 'most', 'common', 'use', 'of', 'opt', '-', 'in', 'e', '-', 'mail', 'marketing', '[SEP]', 'via', 'email', 'whereby', 'the', 'recipient', 'of', 'the', 'advertisement', 'has', 'consent', '##ed', 'to', 'receive', 'it', '.', 'this', 'method', 'is', 'one', 'of', 'several', 'developed', 'by', 'market', '##ers', 'to', 'eliminate', 'the', 'disadvantage', '##s', 'of', 'email', 'marketing', '.', '<', '/', 'p', '>', '<', 'p', '>', 'opt', '-', 'in', 'email', 'marketing', 'may', 'evolve', 'into', 'a', 'technology', 'that', 'uses', 'a', 'hands', '##hak', '##e', 'protocol', 'between', 'the', 'send', '##er', 'and', 'receiver', '.', 'this', 'system', 'is', 'intended', 'to', 'eventually', 'result', 'in', 'a', 'high', 'degree', 'of', 'satisfaction', 'between', 'consumers', 'and', 'market', '##ers', '.', 'if', 'opt', '-', 'in', 'email', 'advertising', 'is', 'used', ',', 'the', 'material', 'that', 'is', 'email', '##ed', 'to', 'consumers', 'will', 'be', '`', '`', 'anticipated', \"'\", \"'\", '.', 'it', 'is', 'assumed', 'that', 'the', 'recipient', 'wants', 'to', 'receive', 'it', ',', 'which', 'makes', 'it', 'unlike', 'un', '##sol', '##ici', '##ted', 'advertisements', 'sent', 'to', 'the', 'consumer', '.', 'ideally', ',', 'opt', '-', 'in', 'email', 'advertisements', 'will', 'be', 'more', 'personal', 'and', 'relevant', 'to', 'the', 'consumer', 'than', 'un', '##tar', '##get', '##ed', 'advertisements', '.', '<', '/', 'p', '>', '<', 'p', '>', 'a', 'common', 'example', 'of', 'permission', 'marketing', 'is', 'a', 'newsletter', 'sent', 'to', 'an', 'advertising', 'firm', \"'\", 's', 'customers', '.', 'such', 'newsletter', '##s', 'inform', 'customers', 'of', 'upcoming', 'events', 'or', 'promotions', ',', 'or', 'new', 'products', '.', 'in', 'this', 'type', 'of', 'advertising', ',', 'a', 'company', 'that', 'wants', 'to', 'send', 'a', 'newsletter', 'to', 'their', 'customers', 'may', 'ask', 'them', 'at', 'the', 'point', 'of', 'purchase', 'if', 'they', 'would', 'like', 'to', 'receive', 'the', 'newsletter', '.', '<', '/', 'p', '>', '<', 'p', '>', 'with', 'a', 'foundation', 'of', 'opted', '-', 'in', 'contact', 'information', 'stored', 'in', 'their', 'database', ',', 'market', '##ers', 'can', 'send', 'out', 'promotional', 'materials', 'automatically', 'using', 'auto', '##res', '##pon', '##ders', '-', '-', 'known', 'as', 'drip', 'marketing', '.', 'they', 'can', 'also', 'segment', 'their', 'promotions', 'to', 'specific', 'market', 'segments', '.', '<', '/', 'p', '>', '<', 'h', '##2', '>', 'legal', 'requirements', '<', '/', 'h', '##2', '>', '<', 'h', '##3', '>', 'australia', '<', '/', 'h', '##3', '>', '<', 'p', '>', 'the', 'australian', 'spa', '##m', 'act', '2003', 'is', 'enforced', 'by', 'the', 'australian', 'communications', 'and', 'media', 'authority', ',', 'widely', 'known', 'as', '`', '`', 'ac', '##ma', \"'\", \"'\", '.', 'the', 'act', 'defines', 'the', 'term', 'un', '##sol', '##ici', '##ted', 'electronic', 'messages', ',', 'states', 'how', 'un', '##su', '##bs', '##cr', '##ibe', 'functions', 'must', 'work', 'for', 'commercial', 'messages', ',', 'and', 'gives', 'other', 'key', 'information', '.', 'fines', 'range', 'with', '3', 'fines', 'of', 'au', '$', '110', ',', '000', 'being', 'issued', 'to', 'virgin', 'blue', 'airlines', '(', '2011', ')', ',', 'tiger', 'airways', 'holdings', 'limited', '(', '2012', ')', 'and', 'cellar', 'master', 'wines', 'pt', '##y', 'limited', '(', '2013', ')', '.', '<', '/', 'p', '>', '<', 'h', '##3', '>', 'canada', '<', '/', 'h', '##3', '>', '<', 'p', '>', 'the', '`', '`', 'canada', 'anti', '-', 'spa', '##m', 'law', \"'\", \"'\", '(', 'cas', '##l', ')', 'went', 'into', 'effect', 'on', 'july', '1', ',', '2014', '.', 'cas', '##l', 'requires', 'an', 'explicit', 'or', 'implicit', 'opt', '-', 'in', 'from', 'users', ',', 'and', 'the', 'maximum', 'fines', 'for', 'non', '##com', '##pl', '##iance', 'are', 'ca', '$', '1', 'million', 'for', 'individuals', 'and', '$', '10', 'million', 'for', 'businesses', '.', '<', '/', 'p', '>', '<', 'h', '##3', '>', 'european', 'union', '[SEP]'], token_to_orig_map=array([-123, -123, -123, -123, -123, -123, -123, -123, -123, -123, -123,\n",
       "       -123, -123, -123, -123, -123, 1815, 1816, 1817, 1818, 1819, 1820,\n",
       "       1821, 1822, 1823, 1824, 1824, 1825, 1826, 1827, 1828, 1829, 1830,\n",
       "       1831, 1832, 1833, 1834, 1835, 1836, 1837, 1837, 1838, 1839, 1840,\n",
       "       1841, 1841, 1842, 1843, 1844, 1845, 1846, 1846, 1846, 1846, 1847,\n",
       "       1847, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856,\n",
       "       1857, 1858, 1859, 1860, 1861, 1861, 1861, 1862, 1863, 1864, 1865,\n",
       "       1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875,\n",
       "       1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1885,\n",
       "       1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896,\n",
       "       1897, 1898, 1899, 1900, 1900, 1901, 1902, 1903, 1904, 1905, 1905,\n",
       "       1906, 1907, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915,\n",
       "       1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1924, 1924,\n",
       "       1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934,\n",
       "       1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945,\n",
       "       1946, 1947, 1948, 1948, 1948, 1948, 1949, 1950, 1951, 1951, 1951,\n",
       "       1951, 1952, 1952, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959,\n",
       "       1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1967, 1968, 1969,\n",
       "       1970, 1971, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979,\n",
       "       1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990,\n",
       "       1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001,\n",
       "       2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\n",
       "       2013, 2014, 2015, 2016, 2017, 2018, 2018, 2018, 2018, 2019, 2019,\n",
       "       2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029,\n",
       "       2030, 2031, 2032, 2033, 2034, 2034, 2035, 2036, 2037, 2038, 2039,\n",
       "       2040, 2041, 2042, 2042, 2042, 2042, 2043, 2043, 2044, 2045, 2046,\n",
       "       2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057,\n",
       "       2058, 2059, 2060, 2060, 2060, 2060, 2061, 2061, 2061, 2061, 2062,\n",
       "       2063, 2064, 2064, 2064, 2064, 2064, 2065, 2065, 2065, 2065, 2066,\n",
       "       2067, 2067, 2067, 2067, 2067, 2068, 2068, 2068, 2069, 2070, 2071,\n",
       "       2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081,\n",
       "       2082, 2083, 2084, 2085, 2086, 2087, 2087, 2088, 2088, 2089, 2089,\n",
       "       2090, 2091, 2092, 2093, 2094, 2095, 2096, 2096, 2096, 2096, 2097,\n",
       "       2098, 2099, 2100, 2101, 2102, 2102, 2102, 2102, 2102, 2103, 2104,\n",
       "       2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115,\n",
       "       2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2123, 2123, 2123,\n",
       "       2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134,\n",
       "       2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145,\n",
       "       2145, 2146, 2147, 2148, 2149, 2150, 2151, 2151, 2151, 2151, 2152,\n",
       "       2152, 2152, 2152, 2153, 2154, 2154, 2154, 2154, 2154, 2155, 2155,\n",
       "       2155, 2156, 2157, 2157, 2158, 2159, 2159, 2159, 2159, 2160, 2161,\n",
       "       2161, 2162, 2163, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170,\n",
       "       2171, 2172, 2173, 2174, 2174, 2175, 2176, 2177, 2178, 2179, 2180,\n",
       "       2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191,\n",
       "       2191, 2191, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199,\n",
       "       2200, 2200, 2201, 2202, 2203, 2204, 2205, 2205, 2205, 2205, 2206,\n",
       "       2206, 2206, 2206, 2207, 2208, -123], dtype=int32), token_is_max_context=array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False]), input_ids=array([  101,  2029,  2003,  1996,  2087,  2691,  2224,  1997, 23569,\n",
       "        1011,  1999,  1041,  1011,  5653,  5821,   102,  3081, 10373,\n",
       "       13557,  1996,  7799,  1997,  1996, 15147,  2038,  9619,  2098,\n",
       "        2000,  4374,  2009,  1012,  2023,  4118,  2003,  2028,  1997,\n",
       "        2195,  2764,  2011,  3006,  2545,  2000, 11027,  1996, 20502,\n",
       "        2015,  1997, 10373,  5821,  1012,  1026,  1013,  1052,  1028,\n",
       "        1026,  1052,  1028, 23569,  1011,  1999, 10373,  5821,  2089,\n",
       "       19852,  2046,  1037,  2974,  2008,  3594,  1037,  2398, 20459,\n",
       "        2063,  8778,  2090,  1996,  4604,  2121,  1998,  8393,  1012,\n",
       "        2023,  2291,  2003,  3832,  2000,  2776,  2765,  1999,  1037,\n",
       "        2152,  3014,  1997,  9967,  2090, 10390,  1998,  3006,  2545,\n",
       "        1012,  2065, 23569,  1011,  1999, 10373,  6475,  2003,  2109,\n",
       "        1010,  1996,  3430,  2008,  2003, 10373,  2098,  2000, 10390,\n",
       "        2097,  2022,  1036,  1036, 11436,  1005,  1005,  1012,  2009,\n",
       "        2003,  5071,  2008,  1996,  7799,  4122,  2000,  4374,  2009,\n",
       "        1010,  2029,  3084,  2009,  4406,  4895, 19454, 28775,  3064,\n",
       "       14389,  2741,  2000,  1996,  7325,  1012, 28946,  1010, 23569,\n",
       "        1011,  1999, 10373, 14389,  2097,  2022,  2062,  3167,  1998,\n",
       "        7882,  2000,  1996,  7325,  2084,  4895,  7559, 18150,  2098,\n",
       "       14389,  1012,  1026,  1013,  1052,  1028,  1026,  1052,  1028,\n",
       "        1037,  2691,  2742,  1997,  6656,  5821,  2003,  1037, 17178,\n",
       "        2741,  2000,  2019,  6475,  3813,  1005,  1055,  6304,  1012,\n",
       "        2107, 17178,  2015, 12367,  6304,  1997,  9046,  2824,  2030,\n",
       "       15365,  1010,  2030,  2047,  3688,  1012,  1999,  2023,  2828,\n",
       "        1997,  6475,  1010,  1037,  2194,  2008,  4122,  2000,  4604,\n",
       "        1037, 17178,  2000,  2037,  6304,  2089,  3198,  2068,  2012,\n",
       "        1996,  2391,  1997,  5309,  2065,  2027,  2052,  2066,  2000,\n",
       "        4374,  1996, 17178,  1012,  1026,  1013,  1052,  1028,  1026,\n",
       "        1052,  1028,  2007,  1037,  3192,  1997, 12132,  1011,  1999,\n",
       "        3967,  2592,  8250,  1999,  2037,  7809,  1010,  3006,  2545,\n",
       "        2064,  4604,  2041, 10319,  4475,  8073,  2478,  8285,  6072,\n",
       "       26029, 13375,  1011,  1011,  2124,  2004, 27304,  5821,  1012,\n",
       "        2027,  2064,  2036,  6903,  2037, 15365,  2000,  3563,  3006,\n",
       "        9214,  1012,  1026,  1013,  1052,  1028,  1026,  1044,  2475,\n",
       "        1028,  3423,  5918,  1026,  1013,  1044,  2475,  1028,  1026,\n",
       "        1044,  2509,  1028,  2660,  1026,  1013,  1044,  2509,  1028,\n",
       "        1026,  1052,  1028,  1996,  2827, 12403,  2213,  2552,  2494,\n",
       "        2003, 16348,  2011,  1996,  2827,  4806,  1998,  2865,  3691,\n",
       "        1010,  4235,  2124,  2004,  1036,  1036,  9353,  2863,  1005,\n",
       "        1005,  1012,  1996,  2552, 11859,  1996,  2744,  4895, 19454,\n",
       "       28775,  3064,  4816,  7696,  1010,  2163,  2129,  4895,  6342,\n",
       "        5910, 26775, 20755,  4972,  2442,  2147,  2005,  3293,  7696,\n",
       "        1010,  1998,  3957,  2060,  3145,  2592,  1012, 21892,  2846,\n",
       "        2007,  1017, 21892,  1997,  8740,  1002,  7287,  1010,  2199,\n",
       "        2108,  3843,  2000,  6261,  2630,  7608,  1006,  2249,  1007,\n",
       "        1010,  6816, 13095,  9583,  3132,  1006,  2262,  1007,  1998,\n",
       "       15423,  3040, 14746, 13866,  2100,  3132,  1006,  2286,  1007,\n",
       "        1012,  1026,  1013,  1052,  1028,  1026,  1044,  2509,  1028,\n",
       "        2710,  1026,  1013,  1044,  2509,  1028,  1026,  1052,  1028,\n",
       "        1996,  1036,  1036,  2710,  3424,  1011, 12403,  2213,  2375,\n",
       "        1005,  1005,  1006, 25222,  2140,  1007,  2253,  2046,  3466,\n",
       "        2006,  2251,  1015,  1010,  2297,  1012, 25222,  2140,  5942,\n",
       "        2019, 13216,  2030, 24655, 23569,  1011,  1999,  2013,  5198,\n",
       "        1010,  1998,  1996,  4555, 21892,  2005,  2512,  9006, 24759,\n",
       "       28335,  2024,  6187,  1002,  1015,  2454,  2005,  3633,  1998,\n",
       "        1002,  2184,  2454,  2005,  5661,  1012,  1026,  1013,  1052,\n",
       "        1028,  1026,  1044,  2509,  1028,  2647,  2586,   102],\n",
       "      dtype=int32), attention_mask=array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True]), token_type_ids=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1], dtype=uint8), paragraph_len=495, start_position=187, end_position=196, long_position=177, short_is_impossible=False, long_is_impossible=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet-mkl==1.6.0\n",
      "  Using cached mxnet_mkl-1.6.0-py2.py3-none-manylinux1_x86_64.whl (76.7 MB)\n",
      "Collecting numpy==1.23.1\n",
      "  Downloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /workspace/tripx/miniconda3/envs/big_data_v2/lib/python3.10/site-packages (from mxnet-mkl==1.6.0) (2.31.0)\n",
      "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet-mkl==1.6.0)\n",
      "  Using cached graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /workspace/tripx/miniconda3/envs/big_data_v2/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/tripx/miniconda3/envs/big_data_v2/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspace/tripx/miniconda3/envs/big_data_v2/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/tripx/miniconda3/envs/big_data_v2/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (2023.7.22)\n",
      "Installing collected packages: numpy, graphviz, mxnet-mkl\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 16] Device or resource busy: '.nfs0000000006380de400000269'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip3 install mxnet-mkl==1.6.0 numpy==1.23.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'which is the most common use of opt-in e-mail marketing',\n",
       "  'short_answers': [{'text': \"a newsletter sent to an advertising firm 's customers\",\n",
       "    'answer_start': 9069}],\n",
       "  'long_answers': [{'text': \"<P> A common example of permission marketing is a newsletter sent to an advertising firm 's customers . Such newsletters inform customers of upcoming events or promotions , or new products . In this type of advertising , a company that wants to send a newsletter to their customers may ask them at the point of purchase if they would like to receive the newsletter . </P>\",\n",
       "    'answer_start': 9021}],\n",
       "  'id': '5655493461695504401',\n",
       "  'short_is_impossible': False,\n",
       "  'long_is_impossible': False,\n",
       "  'crop_start': 418}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'][0]['paragraphs'][0]['qas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['data'][0]['paragraphs'][0]['qas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'which is the most common use of opt-in e-mail marketing',\n",
       " 'short_answers': [{'text': \"a newsletter sent to an advertising firm 's customers\",\n",
       "   'answer_start': 9069}],\n",
       " 'long_answers': [{'text': \"<P> A common example of permission marketing is a newsletter sent to an advertising firm 's customers . Such newsletters inform customers of upcoming events or promotions , or new products . In this type of advertising , a company that wants to send a newsletter to their customers may ask them at the point of purchase if they would like to receive the newsletter . </P>\",\n",
       "   'answer_start': 9021}],\n",
       " 'id': '5655493461695504401',\n",
       " 'short_is_impossible': False,\n",
       " 'long_is_impossible': False,\n",
       " 'crop_start': 418}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'][0]['paragraphs'][0]['qas'][0][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "data_path = '/workspace/tripx/MCS/deep_learning/simplified-nq-train.jsonl'\n",
    "jsonObj = pd.read_json(path_or_buf=data_path, lines=True)\n",
    "\n",
    "print(type(jsonObj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "test_path = '/workspace/tripx/MCS/deep_learning/simplified-nq-test.jsonl'\n",
    "test_data = pd.read_json(path_or_buf=test_path, lines=True)\n",
    "\n",
    "print(type(jsonObj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "example_id                346\n",
       "question_text             346\n",
       "document_text             346\n",
       "long_answer_candidates    346\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document_text             307373\n",
       "long_answer_candidates    307373\n",
       "question_text             307373\n",
       "annotations               307373\n",
       "document_url              307373\n",
       "example_id                307373\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonObj.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train = jsonObj.iloc[:5000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document_text             5000\n",
       "long_answer_candidates    5000\n",
       "question_text             5000\n",
       "annotations               5000\n",
       "document_url              5000\n",
       "example_id                5000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train.to_json(\"simplified-nq-split-train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "tt_path = '/workspace/tripx/MCS/deep_learning/simplified-nq-split-train.json'\n",
    "tt_data = pd.read_json(path_or_buf=tt_path)\n",
    "\n",
    "print(type(tt_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document_text             5000\n",
       "long_answer_candidates    5000\n",
       "question_text             5000\n",
       "annotations               5000\n",
       "document_url              5000\n",
       "example_id                5000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_text</th>\n",
       "      <th>long_answer_candidates</th>\n",
       "      <th>question_text</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_url</th>\n",
       "      <th>example_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'0': 'Email marketing - Wikipedia &lt;H1&gt; Email ...</td>\n",
       "      <td>{'0': [{'start_token': 14, 'top_level': True, ...</td>\n",
       "      <td>{'0': 'which is the most common use of opt-in ...</td>\n",
       "      <td>{'0': [{'yes_no_answer': 'NONE', 'long_answer'...</td>\n",
       "      <td>{'0': 'https://en.wikipedia.org//w/index.php?t...</td>\n",
       "      <td>{'0': 5655493461695504401, '1': 53282124708708...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       document_text  \\\n",
       "0  {'0': 'Email marketing - Wikipedia <H1> Email ...   \n",
       "\n",
       "                              long_answer_candidates  \\\n",
       "0  {'0': [{'start_token': 14, 'top_level': True, ...   \n",
       "\n",
       "                                       question_text  \\\n",
       "0  {'0': 'which is the most common use of opt-in ...   \n",
       "\n",
       "                                         annotations  \\\n",
       "0  {'0': [{'yes_no_answer': 'NONE', 'long_answer'...   \n",
       "\n",
       "                                        document_url  \\\n",
       "0  {'0': 'https://en.wikipedia.org//w/index.php?t...   \n",
       "\n",
       "                                          example_id  \n",
       "0  {'0': 5655493461695504401, '1': 53282124708708...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '/workspace/tripx/MCS/deep_learning/natural-question-answering/simplified-nq-test.jsonl'\n",
    "test_data = pd.read_json(path_or_buf=test_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_text</th>\n",
       "      <th>long_answer_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1220107454853145600</td>\n",
       "      <td>who is the south african high commissioner in ...</td>\n",
       "      <td>High Commission of South Africa , London - wik...</td>\n",
       "      <td>[{'end_token': 136, 'start_token': 18, 'top_le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8777415633185303552</td>\n",
       "      <td>the office episode when they sing to michael</td>\n",
       "      <td>Michael 's Last Dundies - wikipedia &lt;H1&gt; Micha...</td>\n",
       "      <td>[{'end_token': 190, 'start_token': 23, 'top_le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4640548859154538496</td>\n",
       "      <td>what is the main idea of the cross of gold speech</td>\n",
       "      <td>Cross of gold speech - wikipedia &lt;H1&gt; Cross of...</td>\n",
       "      <td>[{'end_token': 165, 'start_token': 12, 'top_le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5316095317154496512</td>\n",
       "      <td>when was i want to sing in opera written</td>\n",
       "      <td>Wilkie Bard - wikipedia &lt;H1&gt; Wilkie Bard &lt;/H1&gt;...</td>\n",
       "      <td>[{'end_token': 105, 'start_token': 8, 'top_lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-8752372642178983936</td>\n",
       "      <td>who does the voices in ice age collision course</td>\n",
       "      <td>Ice Age : Collision Course - Wikipedia &lt;H1&gt; Ic...</td>\n",
       "      <td>[{'end_token': 287, 'start_token': 16, 'top_le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>934950704129185024</td>\n",
       "      <td>how many season of last man standing are there</td>\n",
       "      <td>Last Man Standing ( U.S. TV series ) - wikiped...</td>\n",
       "      <td>[{'end_token': 449, 'start_token': 25, 'top_le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>-7473086642300315648</td>\n",
       "      <td>where does the black forest cake come from</td>\n",
       "      <td>Black Forest gâteau - wikipedia &lt;H1&gt; Black For...</td>\n",
       "      <td>[{'end_token': 95, 'start_token': 18, 'top_lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>4153219118929483776</td>\n",
       "      <td>india won olympic gold medal in hockey 1948</td>\n",
       "      <td>India at the 1948 Summer Olympics - wikipedia ...</td>\n",
       "      <td>[{'end_token': 204, 'start_token': 18, 'top_le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>-7341065354441937920</td>\n",
       "      <td>most viewed youtube music video in 24 hours re...</td>\n",
       "      <td>List of most - viewed online videos in the fir...</td>\n",
       "      <td>[{'end_token': 43, 'start_token': 28, 'top_lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>-3564167666631898112</td>\n",
       "      <td>song hit me with your best shot by pat benatar</td>\n",
       "      <td>Hit Me with Your Best Shot - wikipedia &lt;H1&gt; Hi...</td>\n",
       "      <td>[{'end_token': 244, 'start_token': 16, 'top_le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>346 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              example_id                                      question_text  \\\n",
       "0   -1220107454853145600  who is the south african high commissioner in ...   \n",
       "1    8777415633185303552       the office episode when they sing to michael   \n",
       "2    4640548859154538496  what is the main idea of the cross of gold speech   \n",
       "3   -5316095317154496512           when was i want to sing in opera written   \n",
       "4   -8752372642178983936    who does the voices in ice age collision course   \n",
       "..                   ...                                                ...   \n",
       "341   934950704129185024     how many season of last man standing are there   \n",
       "342 -7473086642300315648         where does the black forest cake come from   \n",
       "343  4153219118929483776        india won olympic gold medal in hockey 1948   \n",
       "344 -7341065354441937920  most viewed youtube music video in 24 hours re...   \n",
       "345 -3564167666631898112     song hit me with your best shot by pat benatar   \n",
       "\n",
       "                                         document_text  \\\n",
       "0    High Commission of South Africa , London - wik...   \n",
       "1    Michael 's Last Dundies - wikipedia <H1> Micha...   \n",
       "2    Cross of gold speech - wikipedia <H1> Cross of...   \n",
       "3    Wilkie Bard - wikipedia <H1> Wilkie Bard </H1>...   \n",
       "4    Ice Age : Collision Course - Wikipedia <H1> Ic...   \n",
       "..                                                 ...   \n",
       "341  Last Man Standing ( U.S. TV series ) - wikiped...   \n",
       "342  Black Forest gâteau - wikipedia <H1> Black For...   \n",
       "343  India at the 1948 Summer Olympics - wikipedia ...   \n",
       "344  List of most - viewed online videos in the fir...   \n",
       "345  Hit Me with Your Best Shot - wikipedia <H1> Hi...   \n",
       "\n",
       "                                long_answer_candidates  \n",
       "0    [{'end_token': 136, 'start_token': 18, 'top_le...  \n",
       "1    [{'end_token': 190, 'start_token': 23, 'top_le...  \n",
       "2    [{'end_token': 165, 'start_token': 12, 'top_le...  \n",
       "3    [{'end_token': 105, 'start_token': 8, 'top_lev...  \n",
       "4    [{'end_token': 287, 'start_token': 16, 'top_le...  \n",
       "..                                                 ...  \n",
       "341  [{'end_token': 449, 'start_token': 25, 'top_le...  \n",
       "342  [{'end_token': 95, 'start_token': 18, 'top_lev...  \n",
       "343  [{'end_token': 204, 'start_token': 18, 'top_le...  \n",
       "344  [{'end_token': 43, 'start_token': 28, 'top_lev...  \n",
       "345  [{'end_token': 244, 'start_token': 16, 'top_le...  \n",
       "\n",
       "[346 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/workspace/tripx/MCS/deep_learning/natural-question-answering/simplified-nq-train.jsonl'\n",
    "train_data = pd.read_json(path_or_buf=train_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_text</th>\n",
       "      <th>long_answer_candidates</th>\n",
       "      <th>question_text</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_url</th>\n",
       "      <th>example_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Email marketing - Wikipedia &lt;H1&gt; Email marketi...</td>\n",
       "      <td>[{'start_token': 14, 'top_level': True, 'end_t...</td>\n",
       "      <td>which is the most common use of opt-in e-mail ...</td>\n",
       "      <td>[{'yes_no_answer': 'NONE', 'long_answer': {'st...</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Em...</td>\n",
       "      <td>5655493461695504401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Mother ( How I Met Your Mother ) - wikiped...</td>\n",
       "      <td>[{'start_token': 28, 'top_level': True, 'end_t...</td>\n",
       "      <td>how i.met your mother who is the mother</td>\n",
       "      <td>[{'yes_no_answer': 'NONE', 'long_answer': {'st...</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Th...</td>\n",
       "      <td>5328212470870865242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Human fertilization - wikipedia &lt;H1&gt; Human fer...</td>\n",
       "      <td>[{'start_token': 14, 'top_level': True, 'end_t...</td>\n",
       "      <td>what type of fertilisation takes place in humans</td>\n",
       "      <td>[{'yes_no_answer': 'NONE', 'long_answer': {'st...</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Hu...</td>\n",
       "      <td>4435104480114867852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>List of National Football League career quarte...</td>\n",
       "      <td>[{'start_token': 28, 'top_level': True, 'end_t...</td>\n",
       "      <td>who had the most wins in the nfl</td>\n",
       "      <td>[{'yes_no_answer': 'NONE', 'long_answer': {'st...</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Li...</td>\n",
       "      <td>5289242154789678439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Roanoke Colony - wikipedia &lt;H1&gt; Roanoke Colony...</td>\n",
       "      <td>[{'start_token': 32, 'top_level': True, 'end_t...</td>\n",
       "      <td>what happened to the lost settlement of roanoke</td>\n",
       "      <td>[{'yes_no_answer': 'NONE', 'long_answer': {'st...</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Ro...</td>\n",
       "      <td>5489863933082811018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307368</th>\n",
       "      <td>The Price Is Right - wikipedia &lt;H1&gt; The Price ...</td>\n",
       "      <td>[{'start_token': 60, 'top_level': True, 'end_t...</td>\n",
       "      <td>who have been the hosts of the price is right</td>\n",
       "      <td>[{'yes_no_answer': 'NONE', 'long_answer': {'st...</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Th...</td>\n",
       "      <td>-1413521544318030897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307369</th>\n",
       "      <td>Mama Told Me Not to Come - wikipedia &lt;H1&gt; Mama...</td>\n",
       "      <td>[{'start_token': 22, 'top_level': True, 'end_t...</td>\n",
       "      <td>who sang the song mama told me not to come</td>\n",
       "      <td>[{'yes_no_answer': 'NONE', 'long_answer': {'st...</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Ma...</td>\n",
       "      <td>3779316254369130993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307370</th>\n",
       "      <td>Jacob Anderson - wikipedia &lt;H1&gt; Jacob Anderson...</td>\n",
       "      <td>[{'start_token': 32, 'top_level': True, 'end_t...</td>\n",
       "      <td>who plays grey worm on game of thrones</td>\n",
       "      <td>[{'yes_no_answer': 'NONE', 'long_answer': {'st...</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Ja...</td>\n",
       "      <td>6455931563852330492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307371</th>\n",
       "      <td>Sodium - vapor lamp - wikipedia &lt;H1&gt; Sodium - ...</td>\n",
       "      <td>[{'start_token': 22, 'top_level': True, 'end_t...</td>\n",
       "      <td>working principle of high pressure sodium vapo...</td>\n",
       "      <td>[{'yes_no_answer': 'NONE', 'long_answer': {'st...</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=So...</td>\n",
       "      <td>-7982911662792302578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307372</th>\n",
       "      <td>Red Hat Enterprise Linux - Wikipedia &lt;H1&gt; Red ...</td>\n",
       "      <td>[{'start_token': 23, 'top_level': True, 'end_t...</td>\n",
       "      <td>what is the latest red hat linux version</td>\n",
       "      <td>[{'yes_no_answer': 'NONE', 'long_answer': {'st...</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Re...</td>\n",
       "      <td>-6253518446509823267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307373 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document_text  \\\n",
       "0       Email marketing - Wikipedia <H1> Email marketi...   \n",
       "1       The Mother ( How I Met Your Mother ) - wikiped...   \n",
       "2       Human fertilization - wikipedia <H1> Human fer...   \n",
       "3       List of National Football League career quarte...   \n",
       "4       Roanoke Colony - wikipedia <H1> Roanoke Colony...   \n",
       "...                                                   ...   \n",
       "307368  The Price Is Right - wikipedia <H1> The Price ...   \n",
       "307369  Mama Told Me Not to Come - wikipedia <H1> Mama...   \n",
       "307370  Jacob Anderson - wikipedia <H1> Jacob Anderson...   \n",
       "307371  Sodium - vapor lamp - wikipedia <H1> Sodium - ...   \n",
       "307372  Red Hat Enterprise Linux - Wikipedia <H1> Red ...   \n",
       "\n",
       "                                   long_answer_candidates  \\\n",
       "0       [{'start_token': 14, 'top_level': True, 'end_t...   \n",
       "1       [{'start_token': 28, 'top_level': True, 'end_t...   \n",
       "2       [{'start_token': 14, 'top_level': True, 'end_t...   \n",
       "3       [{'start_token': 28, 'top_level': True, 'end_t...   \n",
       "4       [{'start_token': 32, 'top_level': True, 'end_t...   \n",
       "...                                                   ...   \n",
       "307368  [{'start_token': 60, 'top_level': True, 'end_t...   \n",
       "307369  [{'start_token': 22, 'top_level': True, 'end_t...   \n",
       "307370  [{'start_token': 32, 'top_level': True, 'end_t...   \n",
       "307371  [{'start_token': 22, 'top_level': True, 'end_t...   \n",
       "307372  [{'start_token': 23, 'top_level': True, 'end_t...   \n",
       "\n",
       "                                            question_text  \\\n",
       "0       which is the most common use of opt-in e-mail ...   \n",
       "1                 how i.met your mother who is the mother   \n",
       "2        what type of fertilisation takes place in humans   \n",
       "3                        who had the most wins in the nfl   \n",
       "4         what happened to the lost settlement of roanoke   \n",
       "...                                                   ...   \n",
       "307368      who have been the hosts of the price is right   \n",
       "307369         who sang the song mama told me not to come   \n",
       "307370             who plays grey worm on game of thrones   \n",
       "307371  working principle of high pressure sodium vapo...   \n",
       "307372           what is the latest red hat linux version   \n",
       "\n",
       "                                              annotations  \\\n",
       "0       [{'yes_no_answer': 'NONE', 'long_answer': {'st...   \n",
       "1       [{'yes_no_answer': 'NONE', 'long_answer': {'st...   \n",
       "2       [{'yes_no_answer': 'NONE', 'long_answer': {'st...   \n",
       "3       [{'yes_no_answer': 'NONE', 'long_answer': {'st...   \n",
       "4       [{'yes_no_answer': 'NONE', 'long_answer': {'st...   \n",
       "...                                                   ...   \n",
       "307368  [{'yes_no_answer': 'NONE', 'long_answer': {'st...   \n",
       "307369  [{'yes_no_answer': 'NONE', 'long_answer': {'st...   \n",
       "307370  [{'yes_no_answer': 'NONE', 'long_answer': {'st...   \n",
       "307371  [{'yes_no_answer': 'NONE', 'long_answer': {'st...   \n",
       "307372  [{'yes_no_answer': 'NONE', 'long_answer': {'st...   \n",
       "\n",
       "                                             document_url           example_id  \n",
       "0       https://en.wikipedia.org//w/index.php?title=Em...  5655493461695504401  \n",
       "1       https://en.wikipedia.org//w/index.php?title=Th...  5328212470870865242  \n",
       "2       https://en.wikipedia.org//w/index.php?title=Hu...  4435104480114867852  \n",
       "3       https://en.wikipedia.org//w/index.php?title=Li...  5289242154789678439  \n",
       "4       https://en.wikipedia.org//w/index.php?title=Ro...  5489863933082811018  \n",
       "...                                                   ...                  ...  \n",
       "307368  https://en.wikipedia.org//w/index.php?title=Th... -1413521544318030897  \n",
       "307369  https://en.wikipedia.org//w/index.php?title=Ma...  3779316254369130993  \n",
       "307370  https://en.wikipedia.org//w/index.php?title=Ja...  6455931563852330492  \n",
       "307371  https://en.wikipedia.org//w/index.php?title=So... -7982911662792302578  \n",
       "307372  https://en.wikipedia.org//w/index.php?title=Re... -6253518446509823267  \n",
       "\n",
       "[307373 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
