{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPCPXY+gcwdPEaCCjFMeS1o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121},"id":"xyHaMq4fQEu-","executionInfo":{"status":"ok","timestamp":1688128699191,"user_tz":-420,"elapsed":909985,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"}},"outputId":"1b00489e-120c-4be3-bc64-bb9a794c3556"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 100000\n","Training finished.\n","\n","Results after 100 episodes:\n","Average timesteps per episode: 13.08\n","Average penalties per episode: 0.0\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"Q-learning.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1ZMpIaCN3KNazOo8elXZnWOYI2uzkFzuq\n","\"\"\"\n","\n","!pip install gym\n","\n","import gym\n","from  google.colab.patches import cv2_imshow\n","\n","env = gym.make(\"Taxi-v3\").env\n","env.reset()\n","cv2_imshow(env.render(mode='rgb_array'))\n","\"\"\"\n","our restructured problem statement (from Gym docs):\n","\n","There are 4 locations (labeled by different letters),\n","and our job is to pick up the passenger at one location and drop him off at another.\n","We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes.\n","There is also a 10 point penalty for illegal pick-up and drop-off actions.\n","\"\"\"\n","env.reset() # reset environment to a new, random state\n","cv2_imshow(env.render(mode='rgb_array'))\n","print(\"Action Space {}\".format(env.action_space))\n","print(\"State Space {}\".format(env.observation_space))\n","\n","\"\"\"\n","We can actually take our illustration above, encode its state,\n","and give it to the environment to render in Gym.\n","Recall that we have the taxi at row 3, column 1, our passenger is at location 2, and our destination is location 0.\n","Using the Taxi-v2 state encoding method, we can do the following:\n","\"\"\"\n","\n","state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n","print(\"State:\", state)\n","\n","env.s = state\n","cv2_imshow(env.render(mode='rgb_array'))\n","\"\"\"\n","The Reward Table\n","When the Taxi environment is created, there is an initial Reward table that's also created, called `P`.\n","We can think of it like a matrix that has the number of states as rows and number of actions as columns, i.e.\n","a states × actions matrix.\n","\n","Since every state is in this matrix, we can see the default reward values assigned to our illustration's state:\n","\"\"\"\n","\n","env.P[328]\n","\n","\n","\n","env.s = 328  # set environment to illustration's state\n","epochs = 0\n","penalties, reward = 0, 0\n","frames = [] # for animation\n","done = False\n","while not done:\n","    action = env.action_space.sample()\n","    state, reward, done, info = env.step(action)\n","    if reward == -10:\n","        penalties += 1\n","    # Put each rendered frame into dict for animation\n","    frames.append({\n","        'frame': env.render(mode='ansi'),\n","        'state': state,\n","        'action': action,\n","        'reward': reward\n","        }\n","    )\n","    epochs += 1\n","\n","print(\"Timesteps taken: {}\".format(epochs))\n","print(\"Penalties incurred: {}\".format(penalties))\n","\n","from IPython.display import clear_output\n","from time import sleep\n","\n","def print_frames(frames):\n","    for i, frame in enumerate(frames):\n","        clear_output(wait=True)\n","        print(frame['frame'])\n","        print(f\"Timestep: {i + 1}\")\n","        print(f\"State: {frame['state']}\")\n","        print(f\"Action: {frame['action']}\")\n","        print(f\"Reward: {frame['reward']}\")\n","        sleep(.1)\n","\n","print_frames(frames)\n","\n","\"\"\"\n","Implementing Q-learning in python\n","Training the Agent\n","First, we'll initialize the Q-table to a 500x6 matrix of zeros:\n","\"\"\"\n","\n","#Implementing Q-learning\n","import numpy as np\n","q_table = np.zeros([env.observation_space.n, env.action_space.n])\n","\n","# Commented out IPython magic to ensure Python compatibility.\n","# %%time\n","# \"\"\"Training the agent\"\"\"\n","#\n","# import random\n","# from IPython.display import clear_output\n","#\n","# # Hyperparameters\n","# alpha = 0.1\n","# gamma = 0.6\n","# epsilon = 0.1\n","#\n","# # For plotting metrics\n","# all_epochs = []\n","# all_penalties = []\n","#\n","# for i in range(1, 100001):\n","#     state = env.reset()\n","#\n","#     epochs, penalties, reward, = 0, 0, 0\n","#     done = False\n","#\n","#     while not done:\n","#         if random.uniform(0, 1) < epsilon:\n","#             action = env.action_space.sample() # Explore action space\n","#         else:\n","#             action = np.argmax(q_table[state]) # Exploit learned values\n","#\n","#         next_state, reward, done, info = env.step(action)\n","#\n","#         old_value = q_table[state, action]\n","#         next_max = np.max(q_table[next_state])\n","#\n","#         new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n","#         q_table[state, action] = new_value\n","#\n","#         if reward == -10:\n","#             penalties += 1\n","#\n","#         state = next_state\n","#         epochs += 1\n","#\n","#     if i % 100 == 0:\n","#         clear_output(wait=True)\n","#         print(f\"Episode: {i}\")\n","#\n","# print(\"Training finished.\\n\")\n","\n","\n","import random\n","from IPython.display import clear_output\n","\n","# Hyperparameters\n","alpha = 0.1\n","gamma = 0.6\n","epsilon = 0.1\n","\n","# For plotting metrics\n","all_epochs = []\n","all_penalties = []\n","\n","for i in range(1, 100001):\n","    state = env.reset()\n","\n","    epochs, penalties, reward, = 0, 0, 0\n","    done = False\n","\n","    while not done:\n","        if random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample() # Explore action space\n","        else:\n","            action = np.argmax(q_table[state]) # Exploit learned values\n","\n","        next_state, reward, done, info = env.step(action)\n","\n","        old_value = q_table[state, action]\n","        next_max = np.max(q_table[next_state])\n","\n","        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n","        q_table[state, action] = new_value\n","\n","        if reward == -10:\n","            penalties += 1\n","\n","        state = next_state\n","        epochs += 1\n","\n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f\"Episode: {i}\")\n","\n","print(\"Training finished.\\n\")\n","\n","\n","\n","\n","\n","#Now that the Q-table has been established over 100,000 episodes,\n","#let's see what the Q-values are at our illustration's state:\n","q_table[328]\n","\n","\"\"\"\n","The max Q-value is \"north\" (-1.971),\n","so it looks like Q-learning has effectively learned the best action to take in our illustration's state!\n","\n","Evaluate agent's performance after Q-learning\n","\"\"\"\n","\n","total_epochs, total_penalties = 0, 0\n","episodes = 100\n","for _ in range(episodes):\n","    state = env.reset()\n","    epochs, penalties, reward = 0, 0, 0\n","    done = False\n","    while not done:\n","        action = np.argmax(q_table[state])\n","        state, reward, done, info = env.step(action)\n","        if reward == -10:\n","            penalties += 1\n","        epochs += 1\n","    total_penalties += penalties\n","    total_epochs += epochs\n","print(f\"Results after {episodes} episodes:\")\n","print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n","print(f\"Average penalties per episode: {total_penalties / episodes}\")"]}]}