{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP+aC/3XpMYjQCbtkvpJLHc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Bài 1: Hãy cài đặt game Cờ caro với giải thuật Reinforcement Learning"],"metadata":{"id":"zcZYEHlwio1v"}},{"cell_type":"code","source":["import numpy as np\n","import random\n","\n","# Kích thước bàn cờ\n","BOARD_SIZE = 3\n","\n","# Hằng số\n","EMPTY = 0\n","PLAYER_X = 1\n","PLAYER_O = 2\n","DRAW = 3\n","\n","# Tạo bàn cờ trống\n","def create_board():\n","    return np.zeros((BOARD_SIZE, BOARD_SIZE))\n","\n","# Kiểm tra trạng thái kết thúc\n","def is_game_over(board):\n","    # Kiểm tra hàng ngang\n","    for i in range(BOARD_SIZE):\n","        if board[i][0] == board[i][1] == board[i][2] != EMPTY:\n","            return True, board[i][0]\n","\n","    # Kiểm tra hàng dọc\n","    for i in range(BOARD_SIZE):\n","        if board[0][i] == board[1][i] == board[2][i] != EMPTY:\n","            return True, board[0][i]\n","\n","    # Kiểm tra đường chéo chính\n","    if board[0][0] == board[1][1] == board[2][2] != EMPTY:\n","        return True, board[0][0]\n","\n","    # Kiểm tra đường chéo phụ\n","    if board[0][2] == board[1][1] == board[2][0] != EMPTY:\n","        return True, board[0][2]\n","\n","    # Kiểm tra hòa\n","    if np.all(board != EMPTY):\n","        return True, DRAW\n","\n","    return False, None\n","\n","# Tìm các nước đi hợp lệ\n","def get_valid_moves(board):\n","    return np.argwhere(board == EMPTY)\n","\n","# Chọn nước đi ngẫu nhiên\n","def choose_random_move(valid_moves):\n","    return random.choice(valid_moves)\n","\n","# Thực hiện nước đi\n","def make_move(board, move, player):\n","    board[move[0]][move[1]] = player\n","\n","# Hàm Q-learning\n","def q_learning(board, num_episodes, learning_rate, discount_factor, exploration_rate):\n","    Q = np.zeros((BOARD_SIZE, BOARD_SIZE, 2))\n","    for episode in range(num_episodes):\n","        current_board = create_board()\n","        while True:\n","            valid_moves = get_valid_moves(current_board)\n","            if np.random.rand() < exploration_rate:\n","                move = choose_random_move(valid_moves)\n","            else:\n","                move_values = [Q[move[0]][move[1]][PLAYER_X - 1] for move in valid_moves]\n","                max_value = max(move_values)\n","                best_moves = [move for move, value in zip(valid_moves, move_values) if value == max_value]\n","                move = random.choice(best_moves)\n","\n","            make_move(current_board, move, PLAYER_X)\n","            game_over, winner = is_game_over(current_board)\n","            if game_over:\n","                if winner == PLAYER_X:\n","                    reward = 1\n","                elif winner == PLAYER_O:\n","                    reward = -1\n","                else:\n","                    reward = 0\n","                Q[move[0]][move[1]][PLAYER_X - 1] += learning_rate * (reward - Q[move[0]][move[1]][PLAYER_X - 1])\n","                break\n","            else:\n","                next_moves = get_valid_moves(current_board)\n","                next_move = random.choice(next_moves)\n","                make_move(current_board, next_move, PLAYER_O)\n","                reward = 0\n","                Q[move[0]][move[1]][PLAYER_X - 1] += learning_rate * (reward + discount_factor * np.max(Q[next_move[0]][next_move[1]]) - Q[move[0]][move[1]][PLAYER_X - 1])\n","    return Q\n","\n","# Hàm chơi với người\n","def play_with_human(Q):\n","    board = create_board()\n","    while True:\n","        print(board)\n","        row = int(input(\"Nhập số hàng: \"))\n","        col = int(input(\"Nhập số cột: \"))\n","        move = (row, col)\n","        make_move(board, move, PLAYER_X)\n","        game_over, winner = is_game_over(board)\n","        if game_over:\n","            print(board)\n","            if winner == PLAYER_X:\n","                print(\"Bạn đã thắng!\")\n","            elif winner == PLAYER_O:\n","                print(\"Bạn đã thua!\")\n","            else:\n","                print(\"Hòa!\")\n","            break\n","        else:\n","            valid_moves = get_valid_moves(board)\n","            best_move = valid_moves[np.argmax(Q[valid_moves[:, 0], valid_moves[:, 1], PLAYER_X - 1])]\n","            make_move(board, best_move, PLAYER_O)\n","            game_over, winner = is_game_over(board)\n","            if game_over:\n","                print(board)\n","                if winner == PLAYER_X:\n","                    print(\"Bạn đã thắng!\")\n","                elif winner == PLAYER_O:\n","                    print(\"Bạn đã thua!\")\n","                else:\n","                    print(\"Hòa!\")\n","                break\n","\n","# Thực hiện Q-learning và chơi với người\n","Q = q_learning(create_board(), num_episodes=1000, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1)\n","play_with_human(Q)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gcCTiaJk1YgC","executionInfo":{"status":"ok","timestamp":1688489839243,"user_tz":-420,"elapsed":14930,"user":{"displayName":"Xuan Tri Pham","userId":"12103575271783886901"}},"outputId":"ac2fdd92-1f70-4507-dd6b-9ff15172558f"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]]\n","Nhập số hàng: 0\n","Nhập số cột: 1\n","[[0. 1. 0.]\n"," [0. 2. 0.]\n"," [0. 0. 0.]]\n","Nhập số hàng: 1 \n","Nhập số cột: 0\n","[[0. 1. 2.]\n"," [1. 2. 0.]\n"," [0. 0. 0.]]\n","Nhập số hàng: 0 \n","Nhập số cột: 0\n","[[1. 1. 2.]\n"," [1. 2. 0.]\n"," [2. 0. 0.]]\n","Bạn đã thua!\n"]}]},{"cell_type":"markdown","source":["## Bài 2: Cải tiến bài 1 với giải thuật Deep Reinforcement Learning"],"metadata":{"id":"jlRpCqMgjRRv"}},{"cell_type":"code","source":[],"metadata":{"id":"FaJSDSl4jQ98"},"execution_count":null,"outputs":[]}]}